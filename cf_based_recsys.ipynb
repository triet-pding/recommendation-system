{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from mysql.connector import Error\n",
    "import mysql.connector\n",
    "import time\n",
    "import structlog\n",
    "import faiss\n",
    "import pickle\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = structlog.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MySQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_mysql() -> mysql.connector:\n",
    "    \"\"\"\n",
    "    Creates and returns a connection to the MySQL database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host='pdingproddbreplica.chioko0q2r4e.us-west-1.rds.amazonaws.com',          # Your MySQL server address (localhost for local)\n",
    "            database='pding_prod_db',  # Your database name\n",
    "            user='readonly',      # Your MySQL username\n",
    "            password='welcomePding'   # Your MySQL password\n",
    "            # Uncomment below if needed:\n",
    "            # port=3306,               # MySQL default port is 3306\n",
    "            # auth_plugin='mysql_native_password'  # If using newer MySQL versions\n",
    "        )\n",
    "        \n",
    "        if connection.is_connected():\n",
    "            logger.info(\"Connected to MySQL database\")\n",
    "            return connection\n",
    "            \n",
    "    except Error as e:\n",
    "        logger.info(f\"Error connecting to MySQL: {e}\")\n",
    "        return None\n",
    "    \n",
    "def close_connection(connection: mysql.connector):\n",
    "    \"\"\"\n",
    "    Close the database connection.\n",
    "    \"\"\"\n",
    "    if connection and connection.is_connected():\n",
    "        connection.close()\n",
    "        logger.info(\"MySQL connection closed\")\n",
    "\n",
    "\n",
    "def execute_multiple_queries_with_timing(connection, queries):\n",
    "    \"\"\"\n",
    "    Execute multiple SQL queries sequentially, return results as a list of pandas DataFrames,\n",
    "    and track execution time for each query and the total process.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    total_start_time = time.perf_counter()\n",
    "    try:\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        for idx, query in enumerate(tqdm(queries, desc=\"Executing queries\")):\n",
    "            query_start_time = time.perf_counter()\n",
    "            try:\n",
    "                cursor.execute(query)\n",
    "                records = cursor.fetchall()\n",
    "                df = pd.DataFrame(records)\n",
    "                dataframes.append(df)\n",
    "                query_end_time = time.perf_counter()\n",
    "                logger.info(f\"Query {idx + 1} executed in {query_end_time - query_start_time:.4f} seconds.\")\n",
    "            except Error as e:\n",
    "                logger.info(f\"Error executing query {idx + 1}: {e}\")\n",
    "                dataframes.append(None)\n",
    "        cursor.close()\n",
    "    except Error as e:\n",
    "        logger.info(f\"Error setting up cursor: {e}\")\n",
    "    total_end_time = time.perf_counter()\n",
    "    logger.info(f\"Total execution time: {total_end_time - total_start_time:.4f} seconds.\")\n",
    "    return dataframes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-05-13 16:56:05\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mConnected to MySQL database   \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing queries:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-05-13 16:56:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mQuery 1 executed in 3.6166 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing queries:  25%|██▌       | 1/4 [00:03<00:10,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-05-13 16:56:10\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mQuery 2 executed in 0.8860 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing queries:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m connection \u001b[38;5;241m=\u001b[39m connect_to_mysql()\n\u001b[1;32m      2\u001b[0m queries \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from videos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from video_rating\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from video_purchase\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from user_followings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 8\u001b[0m dfs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_multiple_queries_with_timing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m videos_df \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m video_rating_df \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mexecute_multiple_queries_with_timing\u001b[0;34m(connection, queries)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(query)\n\u001b[0;32m---> 46\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(records)\n\u001b[1;32m     48\u001b[0m     dataframes\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[0;32m~/pding-recsys/venv/lib/python3.10/site-packages/mysql/connector/cursor_cext.py:977\u001b[0m, in \u001b[0;36mCMySQLCursorDict.fetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetchall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, RowItemType]]:\n\u001b[1;32m    971\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all rows of a query result set.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m        list: A list of dictionaries with all rows of a query\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;124;03m              result set where column names are used as keys.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names, row)) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m res]\n",
      "File \u001b[0;32m~/pding-recsys/venv/lib/python3.10/site-packages/mysql/connector/cursor_cext.py:675\u001b[0m, in \u001b[0;36mCMySQLCursor.fetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39munread_result:\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 675\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nextrow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nextrow[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    677\u001b[0m     rows[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nextrow[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/pding-recsys/venv/lib/python3.10/site-packages/mysql/connector/connection_cext.py:540\u001b[0m, in \u001b[0;36mCMySQLConnection.get_rows\u001b[0;34m(self, count, binary, columns, raw, prep_stmt, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;129;01mand\u001b[39;00m counter \u001b[38;5;241m==\u001b[39m count:\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m row:\n\u001b[1;32m    542\u001b[0m     _eof: Optional[CextEofPacketType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_eof_columns(prep_stmt)[\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meof\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m     ]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "connection = connect_to_mysql()\n",
    "queries = [\n",
    "    \"select * from videos\",\n",
    "    \"select * from video_rating\",\n",
    "    \"select * from video_purchase\",\n",
    "    \"select * from user_followings\"\n",
    "]\n",
    "dfs = execute_multiple_queries_with_timing(connection, queries)\n",
    "videos_df = dfs[0]\n",
    "video_rating_df = dfs[1]\n",
    "video_purchase_df = dfs[2]\n",
    "user_followings_df = dfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of videos_df: (12214, 24)\n",
      "Shape of video_rating_df: (11686, 5)\n",
      "Shape of video_purchase_df: (202279, 13)\n",
      "Shape of user_followings_df: (1696894, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of videos_df: {videos_df.shape}\")\n",
    "print(f\"Shape of video_rating_df: {video_rating_df.shape}\")\n",
    "print(f\"Shape of video_purchase_df: {video_purchase_df.shape}\")\n",
    "print(f\"Shape of user_followings_df: {user_followings_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = videos_df[videos_df['is_deleted'] == 0]\n",
    "user_followings_df = user_followings_df[user_followings_df['is_deleted'] == 0]\n",
    "video_rating_df['last_updated_date'] = pd.to_datetime(video_rating_df['updated_seconds'], unit='s')\n",
    "\n",
    "# Apply efficient renaming to avoid duplicate columns\n",
    "videos_df = videos_df.rename(columns={'duration': 'video_duration'})\n",
    "video_purchase_df = video_purchase_df.rename(columns={\n",
    "    'last_update_date': 'last_purchased_date',\n",
    "    'duration': 'purchase_tier'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8975 entries, 0 to 12213\n",
      "Data columns (total 24 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   id                   8975 non-null   object        \n",
      " 1   description          8975 non-null   object        \n",
      " 2   video_duration       8975 non-null   int64         \n",
      " 3   is_adult             8975 non-null   int64         \n",
      " 4   is_paid              8975 non-null   int64         \n",
      " 5   is_visible           8975 non-null   int64         \n",
      " 6   status               8975 non-null   object        \n",
      " 7   title                8975 non-null   object        \n",
      " 8   trees                2723 non-null   object        \n",
      " 9   updated_time_stamp   8975 non-null   int64         \n",
      " 10  uploaded_time_stamp  8975 non-null   int64         \n",
      " 11  user_id              8975 non-null   object        \n",
      " 12  video_id             8975 non-null   object        \n",
      " 13  rating_visible       8975 non-null   int64         \n",
      " 14  rating_score         2473 non-null   float64       \n",
      " 15  is_deleted           8975 non-null   int64         \n",
      " 16  is_trailer_on        8975 non-null   int64         \n",
      " 17  trailer_id           2165 non-null   object        \n",
      " 18  is_pinned            85 non-null     float64       \n",
      " 19  pinned_at            85 non-null     datetime64[ns]\n",
      " 20  is_preview_on        8975 non-null   int64         \n",
      " 21  trailer_library_id   8975 non-null   object        \n",
      " 22  video_library_id     8975 non-null   object        \n",
      " 23  drm_enable           406 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int64(10), object(10)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "videos_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import psutil  # You may need to pip install this\n",
    "\n",
    "def process_with_tracking(vp_df, v_df, vr_df, uf_df, chunk_size=5000, output_file=\"outputs.csv\"):\n",
    "    # Set up tracking variables\n",
    "    total_rows = len(vp_df)\n",
    "    rows_processed = 0\n",
    "    chunks_processed = 0\n",
    "    start_time = time.time()\n",
    "    results_saved = 0\n",
    "    \n",
    "    # Create or clear output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"\")  # Just clear the file if it exists\n",
    "    \n",
    "    logger.info(f\"Total rows to process: {total_rows}\")\n",
    "    logger.info(f\"Memory usage at start: {psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Process vp dataframe in chunks\n",
    "    for start_idx in range(0, len(vp_df), chunk_size):\n",
    "        try:\n",
    "            chunk_start_time = time.time()\n",
    "            # Get chunk of video_purchase data\n",
    "            end_idx = min(start_idx + chunk_size, len(vp_df))\n",
    "            vp_chunk = vp_df.iloc[start_idx:end_idx].copy()\n",
    "            \n",
    "            # First join\n",
    "            temp_df = pd.merge(\n",
    "                vp_chunk,\n",
    "                v_df[['video_id', 'video_duration', 'rating_score', 'title', 'description']],\n",
    "                on='video_id',\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Check if any rows remain after first join\n",
    "            if len(temp_df) == 0:\n",
    "                logger.info(f\"Chunk {chunks_processed+1}: No matching rows after first join. Skipping.\")\n",
    "                rows_processed += len(vp_chunk)\n",
    "                chunks_processed += 1\n",
    "                continue\n",
    "                \n",
    "            # Second join\n",
    "            temp_df = pd.merge(\n",
    "                temp_df,\n",
    "                vr_df[['rating', 'video_id', 'user_id', 'last_updated_date']],\n",
    "                on=['user_id', 'video_id'],\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Check if any rows remain after second join\n",
    "            if len(temp_df) == 0:\n",
    "                logger.info(f\"Chunk {chunks_processed+1}: No matching rows after second join. Skipping.\")\n",
    "                rows_processed += len(vp_chunk)\n",
    "                chunks_processed += 1\n",
    "                continue\n",
    "            \n",
    "            # Third join\n",
    "            temp_df = pd.merge(\n",
    "                temp_df,\n",
    "                uf_df[['following', 'pd_category', 'pd_language']],\n",
    "                left_on='video_owner_user_id',\n",
    "                right_on='following',\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Remove duplicates and sort\n",
    "            temp_df = temp_df.drop_duplicates()\n",
    "            \n",
    "            # Append to results file\n",
    "            temp_df.to_csv(output_file, mode='a', header=(results_saved==0), index=False)\n",
    "            results_saved += len(temp_df)\n",
    "            \n",
    "            # Update progress tracking\n",
    "            rows_processed += len(vp_chunk)\n",
    "            chunks_processed += 1\n",
    "            \n",
    "            # Print progress\n",
    "            elapsed = time.time() - start_time\n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            memory_usage = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            logger.info(f\"Chunk {chunks_processed}: Processed {rows_processed}/{total_rows} rows ({rows_processed/total_rows*100:.1f}%)\")\n",
    "            logger.info(f\"  - Rows in this chunk result: {len(temp_df)}\")\n",
    "            logger.info(f\"  - Chunk processing time: {chunk_time:.2f}s\")\n",
    "            logger.info(f\"  - Total elapsed time: {elapsed/60:.1f} minutes\")\n",
    "            logger.info(f\"  - Memory usage: {memory_usage:.2f} MB\")\n",
    "            \n",
    "            # Force garbage collection\n",
    "            del temp_df\n",
    "            del vp_chunk\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error processing chunk {chunks_processed+1}: {str(e)}\")\n",
    "            # Save progress info in case of error\n",
    "            with open(\"processing_error_log.txt\", \"a\") as f:\n",
    "                f.write(f\"Error at chunk {chunks_processed+1}, rows {start_idx}-{end_idx}: {str(e)}\\n\")\n",
    "    \n",
    "    logger.info(f\"\\nProcessing complete:\")\n",
    "    logger.info(f\"  - Total rows processed: {rows_processed}\")\n",
    "    logger.info(f\"  - Total rows in results: {results_saved}\")\n",
    "    logger.info(f\"  - Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    logger.info(f\"  - Results saved to: {output_file}\")\n",
    "    \n",
    "    # Read back the final sorted results if needed\n",
    "    # Note: This might be memory-intensive if the result is very large\n",
    "    logger.info(\"Reading and sorting final results...\")\n",
    "    try:\n",
    "        # Read in chunks and sort\n",
    "        sorted_output_file = \"sorted_\" + output_file\n",
    "        # Read first chunk to get header\n",
    "        first_chunk = pd.read_csv(output_file, nrows=1)\n",
    "        header = list(first_chunk.columns)\n",
    "        \n",
    "        # Write sorted chunks\n",
    "        with open(sorted_output_file, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(','.join(header) + '\\n')\n",
    "        \n",
    "        # Process in chunks for sorting\n",
    "        for chunk in pd.read_csv(output_file, chunksize=100000):\n",
    "            chunk_sorted = chunk.sort_values(by='last_purchased_date', ascending=False)\n",
    "            chunk_sorted.to_csv(sorted_output_file, mode='a', header=False, index=False)\n",
    "        \n",
    "        logger.info(f\"Sorted results saved to: {sorted_output_file}\")\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error during final sorting: {str(e)}\")\n",
    "        logger.info(\"You can still access the unsorted results in the original output file.\")\n",
    "    \n",
    "    return results_saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to process: 202279\n",
      "Memory usage at start: 3237.32 MB\n",
      "Chunk 1: Processed 5000/202279 rows (2.5%)\n",
      "  - Rows in this chunk result: 232\n",
      "  - Chunk processing time: 13.94s\n",
      "  - Total elapsed time: 0.2 minutes\n",
      "  - Memory usage: 3180.89 MB\n",
      "Chunk 2: Processed 10000/202279 rows (4.9%)\n",
      "  - Rows in this chunk result: 222\n",
      "  - Chunk processing time: 12.26s\n",
      "  - Total elapsed time: 0.4 minutes\n",
      "  - Memory usage: 3180.89 MB\n",
      "Chunk 3: Processed 15000/202279 rows (7.4%)\n",
      "  - Rows in this chunk result: 239\n",
      "  - Chunk processing time: 14.25s\n",
      "  - Total elapsed time: 0.7 minutes\n",
      "  - Memory usage: 3180.89 MB\n",
      "Chunk 4: Processed 20000/202279 rows (9.9%)\n",
      "  - Rows in this chunk result: 260\n",
      "  - Chunk processing time: 16.27s\n",
      "  - Total elapsed time: 0.9 minutes\n",
      "  - Memory usage: 3180.89 MB\n",
      "Chunk 5: Processed 25000/202279 rows (12.4%)\n",
      "  - Rows in this chunk result: 244\n",
      "  - Chunk processing time: 14.40s\n",
      "  - Total elapsed time: 1.2 minutes\n",
      "  - Memory usage: 3180.90 MB\n",
      "Chunk 6: Processed 30000/202279 rows (14.8%)\n",
      "  - Rows in this chunk result: 235\n",
      "  - Chunk processing time: 12.82s\n",
      "  - Total elapsed time: 1.4 minutes\n",
      "  - Memory usage: 3180.90 MB\n",
      "Chunk 7: Processed 35000/202279 rows (17.3%)\n",
      "  - Rows in this chunk result: 223\n",
      "  - Chunk processing time: 13.08s\n",
      "  - Total elapsed time: 1.6 minutes\n",
      "  - Memory usage: 3180.90 MB\n",
      "Chunk 8: Processed 40000/202279 rows (19.8%)\n",
      "  - Rows in this chunk result: 267\n",
      "  - Chunk processing time: 14.09s\n",
      "  - Total elapsed time: 1.9 minutes\n",
      "  - Memory usage: 3180.90 MB\n",
      "Chunk 9: Processed 45000/202279 rows (22.2%)\n",
      "  - Rows in this chunk result: 233\n",
      "  - Chunk processing time: 12.56s\n",
      "  - Total elapsed time: 2.1 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 10: Processed 50000/202279 rows (24.7%)\n",
      "  - Rows in this chunk result: 226\n",
      "  - Chunk processing time: 12.13s\n",
      "  - Total elapsed time: 2.3 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 11: Processed 55000/202279 rows (27.2%)\n",
      "  - Rows in this chunk result: 236\n",
      "  - Chunk processing time: 13.63s\n",
      "  - Total elapsed time: 2.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 12: Processed 60000/202279 rows (29.7%)\n",
      "  - Rows in this chunk result: 218\n",
      "  - Chunk processing time: 13.24s\n",
      "  - Total elapsed time: 2.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 13: Processed 65000/202279 rows (32.1%)\n",
      "  - Rows in this chunk result: 239\n",
      "  - Chunk processing time: 14.56s\n",
      "  - Total elapsed time: 3.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 14: Processed 70000/202279 rows (34.6%)\n",
      "  - Rows in this chunk result: 258\n",
      "  - Chunk processing time: 16.07s\n",
      "  - Total elapsed time: 3.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 15: Processed 75000/202279 rows (37.1%)\n",
      "  - Rows in this chunk result: 261\n",
      "  - Chunk processing time: 16.10s\n",
      "  - Total elapsed time: 3.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 16: Processed 80000/202279 rows (39.5%)\n",
      "  - Rows in this chunk result: 242\n",
      "  - Chunk processing time: 16.78s\n",
      "  - Total elapsed time: 3.8 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 17: Processed 85000/202279 rows (42.0%)\n",
      "  - Rows in this chunk result: 241\n",
      "  - Chunk processing time: 15.78s\n",
      "  - Total elapsed time: 4.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 18: Processed 90000/202279 rows (44.5%)\n",
      "  - Rows in this chunk result: 249\n",
      "  - Chunk processing time: 14.99s\n",
      "  - Total elapsed time: 4.3 minutes\n",
      "  - Memory usage: 3212.30 MB\n",
      "Chunk 19: Processed 95000/202279 rows (47.0%)\n",
      "  - Rows in this chunk result: 257\n",
      "  - Chunk processing time: 16.30s\n",
      "  - Total elapsed time: 4.6 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 20: Processed 100000/202279 rows (49.4%)\n",
      "  - Rows in this chunk result: 248\n",
      "  - Chunk processing time: 15.68s\n",
      "  - Total elapsed time: 4.8 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 21: Processed 105000/202279 rows (51.9%)\n",
      "  - Rows in this chunk result: 235\n",
      "  - Chunk processing time: 12.14s\n",
      "  - Total elapsed time: 5.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 22: Processed 110000/202279 rows (54.4%)\n",
      "  - Rows in this chunk result: 236\n",
      "  - Chunk processing time: 13.12s\n",
      "  - Total elapsed time: 5.3 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 23: Processed 115000/202279 rows (56.9%)\n",
      "  - Rows in this chunk result: 227\n",
      "  - Chunk processing time: 12.98s\n",
      "  - Total elapsed time: 5.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 24: Processed 120000/202279 rows (59.3%)\n",
      "  - Rows in this chunk result: 239\n",
      "  - Chunk processing time: 13.37s\n",
      "  - Total elapsed time: 5.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 25: Processed 125000/202279 rows (61.8%)\n",
      "  - Rows in this chunk result: 256\n",
      "  - Chunk processing time: 16.00s\n",
      "  - Total elapsed time: 6.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 26: Processed 130000/202279 rows (64.3%)\n",
      "  - Rows in this chunk result: 254\n",
      "  - Chunk processing time: 14.93s\n",
      "  - Total elapsed time: 6.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 27: Processed 135000/202279 rows (66.7%)\n",
      "  - Rows in this chunk result: 267\n",
      "  - Chunk processing time: 16.03s\n",
      "  - Total elapsed time: 6.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 28: Processed 140000/202279 rows (69.2%)\n",
      "  - Rows in this chunk result: 259\n",
      "  - Chunk processing time: 15.20s\n",
      "  - Total elapsed time: 6.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 29: Processed 145000/202279 rows (71.7%)\n",
      "  - Rows in this chunk result: 254\n",
      "  - Chunk processing time: 15.48s\n",
      "  - Total elapsed time: 7.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 30: Processed 150000/202279 rows (74.2%)\n",
      "  - Rows in this chunk result: 223\n",
      "  - Chunk processing time: 12.80s\n",
      "  - Total elapsed time: 7.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 31: Processed 155000/202279 rows (76.6%)\n",
      "  - Rows in this chunk result: 228\n",
      "  - Chunk processing time: 13.17s\n",
      "  - Total elapsed time: 7.4 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 32: Processed 160000/202279 rows (79.1%)\n",
      "  - Rows in this chunk result: 245\n",
      "  - Chunk processing time: 14.97s\n",
      "  - Total elapsed time: 7.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 33: Processed 165000/202279 rows (81.6%)\n",
      "  - Rows in this chunk result: 264\n",
      "  - Chunk processing time: 15.00s\n",
      "  - Total elapsed time: 7.9 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 34: Processed 170000/202279 rows (84.0%)\n",
      "  - Rows in this chunk result: 252\n",
      "  - Chunk processing time: 17.56s\n",
      "  - Total elapsed time: 8.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 35: Processed 175000/202279 rows (86.5%)\n",
      "  - Rows in this chunk result: 242\n",
      "  - Chunk processing time: 14.06s\n",
      "  - Total elapsed time: 8.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 36: Processed 180000/202279 rows (89.0%)\n",
      "  - Rows in this chunk result: 255\n",
      "  - Chunk processing time: 14.72s\n",
      "  - Total elapsed time: 8.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 37: Processed 185000/202279 rows (91.5%)\n",
      "  - Rows in this chunk result: 232\n",
      "  - Chunk processing time: 13.71s\n",
      "  - Total elapsed time: 8.9 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 38: Processed 190000/202279 rows (93.9%)\n",
      "  - Rows in this chunk result: 263\n",
      "  - Chunk processing time: 15.14s\n",
      "  - Total elapsed time: 9.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 39: Processed 195000/202279 rows (96.4%)\n",
      "  - Rows in this chunk result: 252\n",
      "  - Chunk processing time: 14.83s\n",
      "  - Total elapsed time: 9.4 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 40: Processed 200000/202279 rows (98.9%)\n",
      "  - Rows in this chunk result: 253\n",
      "  - Chunk processing time: 15.59s\n",
      "  - Total elapsed time: 9.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 41: Processed 202279/202279 rows (100.0%)\n",
      "  - Rows in this chunk result: 102\n",
      "  - Chunk processing time: 5.68s\n",
      "  - Total elapsed time: 9.8 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "\n",
      "Processing complete:\n",
      "  - Total rows processed: 202279\n",
      "  - Total rows in results: 9868\n",
      "  - Total time: 9.8 minutes\n",
      "  - Results saved to: outputs.csv\n",
      "Reading and sorting final results...\n",
      "Sorted results saved to: sorted_outputs.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9868"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_with_tracking(\n",
    "    v_df=videos_df,\n",
    "    vp_df=video_purchase_df,\n",
    "    uf_df=user_followings_df,\n",
    "    vr_df=video_rating_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(\"/home/cyrilng/pding-recsys/data/master_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10111 entries, 0 to 10110\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   last_purchased_date  10111 non-null  object \n",
      " 1   trees_consumed       10111 non-null  float64\n",
      " 2   user_id              10111 non-null  object \n",
      " 3   video_id             10111 non-null  object \n",
      " 4   purchase_tier        10111 non-null  object \n",
      " 5   video_duration       10111 non-null  int64  \n",
      " 6   wilson_score         10111 non-null  float64\n",
      " 7   title                10111 non-null  object \n",
      " 8   description          10111 non-null  object \n",
      " 9   rating               10111 non-null  int64  \n",
      " 10  last_updated_date    10111 non-null  object \n",
      " 11  pd_category          10111 non-null  object \n",
      " 12  pd_language          10090 non-null  object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column and rename\n",
    "master_df.drop(columns=['drm_fee', 'discount_percentage_applied', 'package_purchase_id', \n",
    "                        'is_replacement_of_deleted_video', 'following','is_refunded', \n",
    "                        'expiry_date', 'id', 'video_owner_user_id'], inplace=True)\n",
    "master_df.rename(columns={'rating_score': 'wilson_score'}, inplace=True)\n",
    "master_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 4086\n",
      "Number of unique videos: 2447\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Number of unique users: {master_df['user_id'].nunique()}\")\n",
    "logger.info(f\"Number of unique videos: {master_df['video_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9810 entries, 0 to 9867\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   last_purchased_date  9810 non-null   object \n",
      " 1   trees_consumed       9810 non-null   float64\n",
      " 2   user_id              9810 non-null   object \n",
      " 3   video_id             9810 non-null   object \n",
      " 4   purchase_tier        9810 non-null   object \n",
      " 5   video_duration       9810 non-null   int64  \n",
      " 6   wilson_score         9810 non-null   float64\n",
      " 7   title                9810 non-null   object \n",
      " 8   description          9810 non-null   object \n",
      " 9   rating               9810 non-null   int64  \n",
      " 10  last_updated_date    9810 non-null   object \n",
      " 11  pd_category          9810 non-null   object \n",
      " 12  pd_language          9810 non-null   object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "master_df = master_df.dropna(subset=[\"pd_category\", \"pd_language\"])\n",
    "master_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from konlpy.tag import Okt  # Korean language processor\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedFiltering:\n",
    "    \"\"\"\n",
    "    Content-based filtering recommendation system for items with Korean metadata.\n",
    "    Specifically handles Korean text in 'title' and 'description' attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        #  Initialize Korean text processor\n",
    "        self.okt = Okt() \n",
    "        self.korean_stopwords = self._load_korean_stopwords()\n",
    "        \n",
    "        # Vector database\n",
    "        self.index = None\n",
    "        self.id_mapping = {}\n",
    "        \n",
    "        # Initialize transformers\n",
    "        self.text_vectorizer = None\n",
    "        self.numerical_scaler = MinMaxScaler()\n",
    "        self.categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "        logger.info(\"Content-based Recommender initialized\")\n",
    "        \n",
    "    def _load_korean_stopwords(self):\n",
    "        \"\"\"Load Korean stopwords or use a default set if file not available\"\"\"\n",
    "        logger.info(\"Load Korean stopwords list\")\n",
    "        try:\n",
    "            with open('korean_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "                return set(f.read().splitlines())\n",
    "        except FileNotFoundError:\n",
    "            # Default basic Korean stopwords\n",
    "            return {'이', '그', '저', '것', '수', '등', '들', '및', '에서', '으로', '를', '에', '의', '가', '은', '는', '이런', '저런', '그런'}\n",
    "    \n",
    "    def _tokenize_korean_text(self, text):\n",
    "        \"\"\"Preprocess Korean text with specialized handling\"\"\"\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Normalize text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Korean, English, numbers\n",
    "        text = re.sub(r'[^\\wㄱ-ㅎㅏ-ㅣ가-힣 ]', ' ', text)\n",
    "        \n",
    "        # Tokenize Korean text and select only nouns, adjectives, verbs\n",
    "        tokens = self.okt.pos(text)\n",
    "        filtered_tokens = [word for word, pos in tokens if (pos in ['Noun', 'Adjective', 'Verb'] and \n",
    "                                                           len(word) > 1 and \n",
    "                                                           word not in self.korean_stopwords)]\n",
    "        \n",
    "        return ' '.join(filtered_tokens)\n",
    "\n",
    "    def preprocess_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess Korean text columns (title and description)\"\"\"\n",
    "        logger.info(f\"Preprocessing text for {len(df)} videos\")\n",
    "        # Create copies to avoid modifying the original dataframe\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Tokenize Korean text\n",
    "        df_processed['title_tokenized'] = df_processed['title'].fillna(\"\").apply(self._tokenize_korean_text)\n",
    "        df_processed['description_tokenized'] = df_processed['description'].fillna(\"\").apply(self._tokenize_korean_text)\n",
    "        \n",
    "        # Combine text features\n",
    "        df_processed['text_combined'] = df_processed['title_tokenized'] + \" \" + df_processed['description_tokenized']\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def extract_text_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Extract TF-IDF features from preprocessed text\"\"\"\n",
    "        logger.info(\"Extracting text features\")\n",
    "        if self.text_vectorizer is None:\n",
    "            # Initialize and fit vectorizer if not already done\n",
    "            self.text_vectorizer = TfidfVectorizer(\n",
    "                min_df=2, \n",
    "                max_df=0.95, \n",
    "                max_features=5000, \n",
    "                ngram_range=(1, 2), \n",
    "                sublinear_tf=True\n",
    "            )\n",
    "            text_features = self.text_vectorizer.fit_transform(df['text_combined'])\n",
    "        else:\n",
    "            # Use pre-trained vectorizer\n",
    "            text_features = self.text_vectorizer.transform(df['text_combined'])\n",
    "            \n",
    "        return text_features\n",
    "    \n",
    "    def extract_metadata_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Extract and encode numerical and categorical metadata features\"\"\"\n",
    "        logger.info(\"Extracting metadata features\")\n",
    "        # Handle numerical features\n",
    "        numerical_features = df[['trees_consumed', 'video_duration']].values\n",
    "        scaled_numerical = self.numerical_scaler.fit_transform(numerical_features)\n",
    "        \n",
    "        # Handle categorical features\n",
    "        categorical_features = df[['purchase_tier', 'pd_category']].values\n",
    "        encoded_categorical = self.categorical_encoder.fit_transform(categorical_features).toarray()\n",
    "        \n",
    "        # Combine all metadata features\n",
    "        metadata_features = np.hstack((scaled_numerical, encoded_categorical))\n",
    "        \n",
    "        return metadata_features\n",
    "    \n",
    "    def combine_features(self, text_features: np.ndarray, metadata_features: np.ndarray, \n",
    "                     text_weight: float = 0.7) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            text_features: Sparse or dense text feature matrix\n",
    "            metadata_features: Dense metadata feature matrix\n",
    "            text_weight: Weight to apply to text features (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            Combined feature matrix\n",
    "        \"\"\"\n",
    "        logger.info(f\"Combining features with text_weight={text_weight}\")\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Get dimensions\n",
    "        n_samples = metadata_features.shape[0]\n",
    "        \n",
    "        # For metadata features - normalize in-place if possible\n",
    "        metadata_norm = np.linalg.norm(metadata_features, axis=1, keepdims=True)\n",
    "        np.divide(metadata_features, np.maximum(metadata_norm, 1e-10), out=metadata_features)\n",
    "        \n",
    "        # Apply weight to metadata\n",
    "        metadata_features *= (1 - text_weight)\n",
    "        \n",
    "        # Process text features efficiently based on their type\n",
    "        if isinstance(text_features, scipy.sparse.spmatrix):\n",
    "            # For sparse matrices, we handle differently to preserve memory\n",
    "            # Normalize sparse matrix (this preserves sparsity)\n",
    "            text_squared = text_features.copy()\n",
    "            text_squared.data **= 2\n",
    "            text_norm = np.sqrt(text_squared.sum(axis=1).A1)\n",
    "            \n",
    "            # Create a diagonal matrix of normalization factors\n",
    "            normalizer = scipy.sparse.diags(1.0 / np.maximum(text_norm, 1e-10))\n",
    "            \n",
    "            # Normalize and apply weight (still sparse)\n",
    "            text_normalized = normalizer @ text_features\n",
    "            text_normalized *= text_weight\n",
    "            \n",
    "            # Now we need to combine - convert text to dense only at the final step\n",
    "            logger.info(\"Converting sparse text features to dense for final combination\")\n",
    "            combined_features = np.hstack((text_normalized.toarray(), metadata_features))\n",
    "        else:\n",
    "            # If already dense, normalize with np operations\n",
    "            text_norm = np.linalg.norm(text_features, axis=1, keepdims=True)\n",
    "            text_features /= np.maximum(text_norm, 1e-10)\n",
    "            text_features *= text_weight\n",
    "            \n",
    "            # Combine\n",
    "            combined_features = np.hstack((text_features, metadata_features))\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        logger.info(f\"Total execution time: {end_time - start_time:.4f} seconds. \" \n",
    "                f\"Combined shape: {combined_features.shape}\")\n",
    "        \n",
    "        # Report memory usage\n",
    "        mem_usage = combined_features.nbytes / (1024 * 1024)\n",
    "        logger.info(f\"Memory usage of combined features: {mem_usage:.2f} MB\")\n",
    "        \n",
    "        return combined_features\n",
    "    \n",
    "    def build_faiss_index(self, feature_matrix: np.ndarray) -> None:\n",
    "        \"\"\"Build FAISS index for fast similarity search\"\"\"\n",
    "        logger.info(f\"Building FAISS index with {feature_matrix.shape[0]} videos\")\n",
    "        \n",
    "        # Convert to float32 as required by FAISS\n",
    "        features_float32 = feature_matrix.astype(np.float32)\n",
    "        \n",
    "        # Create and train index\n",
    "        dimension = features_float32.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "        \n",
    "        # Add vectors to the index\n",
    "        self.index.add(features_float32)\n",
    "        \n",
    "        logger.info(f\"FAISS index built with {self.index.ntotal} vectors\")\n",
    "\n",
    "    def fit(self, video_data: pd.DataFrame) -> None:\n",
    "        \"\"\"Fit the recommendation model on the provided video data\"\"\"\n",
    "        logger.info(f\"Fitting model on {len(video_data)} videos\")\n",
    "        \n",
    "        # Store original video IDs for mapping\n",
    "        original_indices = video_data.index.tolist()\n",
    "        \n",
    "        # Preprocess text\n",
    "        processed_df = self.preprocess_text(video_data)\n",
    "        \n",
    "        # Extract features\n",
    "        text_features = self.extract_text_features(processed_df)\n",
    "        metadata_features = self.extract_metadata_features(processed_df)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = self.combine_features(text_features, metadata_features)\n",
    "        \n",
    "        # Build search index\n",
    "        self.build_faiss_index(combined_features)\n",
    "        \n",
    "        # Create mapping from FAISS index to original video IDs\n",
    "        self.id_mapping = {i: original_indices[i] for i in range(len(original_indices))}\n",
    "        \n",
    "        # Save models\n",
    "        self.save_models()\n",
    "        \n",
    "        logger.info(\"Model fitting completed\")\n",
    "\n",
    "    def find_similar_videos(self, video_id: int, video_data: pd.DataFrame, top_n: int = 10) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Find videos similar to the given video ID\"\"\"\n",
    "        # # Cache key for this query\n",
    "        # cache_key = f\"sim_videos:{video_id}:{top_n}\"\n",
    "        \n",
    "        # # Try to get from cache first\n",
    "        # if self.use_cache:\n",
    "        #     cached_result = self.cache.get(cache_key)\n",
    "        #     if cached_result:\n",
    "        #         CACHE_HIT_COUNTER.inc()\n",
    "        #         logger.info(f\"Cache hit for video_id={video_id}\")\n",
    "        #         return pickle.loads(cached_result)\n",
    "        \n",
    "        logger.info(f\"Finding {top_n} videos similar to video_id={video_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Get the index of the video in our processed data\n",
    "            video_idx = list(self.id_mapping.values()).index(video_id)\n",
    "            \n",
    "            # Get the feature vector for this video\n",
    "            query_vector = np.array([self.index.reconstruct(video_idx)]).astype(np.float32)\n",
    "            \n",
    "            # Search for similar videos\n",
    "            k = top_n + 1  # +1 because the video itself will be included\n",
    "            distances, indices = self.index.search(query_vector, k)\n",
    "            \n",
    "            # Convert to list of (video_id, similarity_score) tuples\n",
    "            # Skip the first result (which is the query video itself)\n",
    "            similar_videos = []\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if self.id_mapping[idx] != video_id:  # Skip the query video\n",
    "                    # Convert distance to similarity score (1 / (1 + distance))\n",
    "                    similarity = 1 / (1 + distances[0][i])\n",
    "                    similar_videos.append((self.id_mapping[idx], float(similarity)))\n",
    "                \n",
    "                if len(similar_videos) == top_n:\n",
    "                    break\n",
    "            \n",
    "            # Cache the result\n",
    "            # if self.use_cache:\n",
    "            #     self.cache.setex(cache_key, self.cache_ttl, pickle.dumps(similar_videos))\n",
    "            \n",
    "            return similar_videos\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error finding similar videos: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "    def save_models(self) -> None:\n",
    "        \"\"\"Save trained models and preprocessors to disk\"\"\"\n",
    "        logger.info(f\"Saving models to {self.model_dir}\")\n",
    "        \n",
    "        # Save text vectorizer\n",
    "        with open(os.path.join(self.model_dir, \"text_vectorizer.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.text_vectorizer, f)\n",
    "        \n",
    "        # Save numerical scaler\n",
    "        with open(os.path.join(self.model_dir, \"numerical_scaler.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.numerical_scaler, f)\n",
    "        \n",
    "        # Save categorical encoder\n",
    "        with open(os.path.join(self.model_dir, \"categorical_encoder.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.categorical_encoder, f)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, os.path.join(self.model_dir, \"faiss_index.bin\"))\n",
    "        \n",
    "        # Save ID mapping\n",
    "        with open(os.path.join(self.model_dir, \"id_mapping.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.id_mapping, f)\n",
    "            \n",
    "        logger.info(\"Models saved successfully\")\n",
    "    \n",
    "    def load_models(self) -> None:\n",
    "        \"\"\"Load trained models and preprocessors from disk\"\"\"\n",
    "        logger.info(f\"Loading models from {self.model_dir}\")\n",
    "        \n",
    "        try:\n",
    "            # Load text vectorizer\n",
    "            with open(os.path.join(self.model_dir, \"text_vectorizer.pkl\"), \"rb\") as f:\n",
    "                self.text_vectorizer = pickle.load(f)\n",
    "            \n",
    "            # Load numerical scaler\n",
    "            with open(os.path.join(self.model_dir, \"numerical_scaler.pkl\"), \"rb\") as f:\n",
    "                self.numerical_scaler = pickle.load(f)\n",
    "            \n",
    "            # Load categorical encoder\n",
    "            with open(os.path.join(self.model_dir, \"categorical_encoder.pkl\"), \"rb\") as f:\n",
    "                self.categorical_encoder = pickle.load(f)\n",
    "            \n",
    "            # Load FAISS index\n",
    "            self.index = faiss.read_index(os.path.join(self.model_dir, \"faiss_index.bin\"))\n",
    "            \n",
    "            # Load ID mapping\n",
    "            with open(os.path.join(self.model_dir, \"id_mapping.pkl\"), \"rb\") as f:\n",
    "                self.id_mapping = pickle.load(f)\n",
    "                \n",
    "            logger.info(\"Models loaded successfully\")\n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading models: {str(e)}\")\n",
    "            return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10111 entries, 0 to 10110\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   last_purchased_date  10111 non-null  object \n",
      " 1   trees_consumed       10111 non-null  float64\n",
      " 2   user_id              10111 non-null  object \n",
      " 3   video_id             10111 non-null  object \n",
      " 4   purchase_tier        10111 non-null  object \n",
      " 5   video_duration       10111 non-null  int64  \n",
      " 6   wilson_score         10111 non-null  float64\n",
      " 7   title                10111 non-null  object \n",
      " 8   description          10111 non-null  object \n",
      " 9   rating               10111 non-null  int64  \n",
      " 10  last_updated_date    10111 non-null  object \n",
      " 11  pd_category          10111 non-null  object \n",
      " 12  pd_language          10090 non-null  object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-05-14 17:31:13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLoad Korean stopwords list    \u001b[0m\n",
      "\u001b[2m2025-05-14 17:31:13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mContent-based Recommender initialized\u001b[0m\n",
      "\u001b[2m2025-05-14 17:31:13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mFitting model on 10111 videos \u001b[0m\n",
      "\u001b[2m2025-05-14 17:31:13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPreprocessing text for 10111 videos\u001b[0m\n",
      "\u001b[2m2025-05-14 17:34:17\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExtracting text features      \u001b[0m\n",
      "\u001b[2m2025-05-14 17:34:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExtracting metadata features  \u001b[0m\n",
      "\u001b[2m2025-05-14 17:34:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCombining features with text_weight=0.7\u001b[0m\n",
      "\u001b[2m2025-05-14 17:34:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mConverting sparse text features to dense for final combination\u001b[0m\n",
      "\u001b[2m2025-05-14 17:34:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mTotal execution time: 0.5700 seconds. Combined shape: (10111, 5016)\u001b[0m\n",
      "\u001b[2m2025-05-14 17:34:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mMemory usage of combined features: 386.94 MB\u001b[0m\n",
      "\u001b[2m2025-05-14 17:34:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mBuilding FAISS index with 10111 videos\u001b[0m\n",
      "\u001b[2m2025-05-14 17:34:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mFAISS index built with 10111 vectors\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ContentBasedFiltering' object has no attribute 'model_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cf_recommender \u001b[38;5;241m=\u001b[39m ContentBasedFiltering()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcf_recommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaster_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 205\u001b[0m, in \u001b[0;36mContentBasedFiltering.fit\u001b[0;34m(self, video_data)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_mapping \u001b[38;5;241m=\u001b[39m {i: original_indices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(original_indices))}\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Save models\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel fitting completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 259\u001b[0m, in \u001b[0;36mContentBasedFiltering.save_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_models\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save trained models and preprocessors to disk\"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving models to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# Save text vectorizer\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_vectorizer.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ContentBasedFiltering' object has no attribute 'model_dir'"
     ]
    }
   ],
   "source": [
    "cf_recommender = ContentBasedFiltering()\n",
    "cf_recommender.fit(video_data=master_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-to-Item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (11629, 4)\n",
      "Number of unique users: 4831\n",
      "Number of unique videos: 2924\n",
      "Rating distribution:\n",
      "rating\n",
      "5    0.738929\n",
      "1    0.118497\n",
      "4    0.072577\n",
      "3    0.046952\n",
      "2    0.023046\n",
      "Name: proportion, dtype: float64\n",
      "Starting model fitting...\n",
      "Creating user and item mappings...\n",
      "Found 4158 unique users and 2608 unique videos\n",
      "Building user-item matrix...\n",
      "Created user-item matrix of shape (4158, 2608)\n",
      "Building item similarity matrix...\n",
      "Processed 1000/2608 items... (152.02 sec)\n",
      "Processed 2000/2608 items... (310.01 sec)\n",
      "Item similarity matrix built in 411.72 seconds\n",
      "Model fitting completed in 411.74 seconds\n",
      "Model saved to item_cf_model.pkl\n",
      "Test RMSE: 1.118815692958474\n",
      "Top 5 recommendations for user R7DePNyqQDUQOT1MJhIawC2IN323:\n",
      "  Video ee8e8b74-c446-485e-90ab-5285d8b6607c: Score 1.7344\n",
      "  Video e3388292-edde-4a1a-9e7c-5c59973331b2: Score 1.4222\n",
      "  Video b8fd416f-ba7a-4d9c-946d-18e001fbe03f: Score 1.4222\n",
      "  Video 4d13a11e-2206-4a97-a01b-53b9052858d4: Score 1.4222\n",
      "  Video 7274c94e-ca0c-4ea2-8704-0ac3b60bce16: Score 1.4222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "class ItemBasedCF:\n",
    "    \"\"\"\n",
    "    Item-based Collaborative Filtering recommendation system\n",
    "    specifically designed for video recommendations with user ratings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, top_n_similar=10):\n",
    "        \"\"\"\n",
    "        Initialize the recommender system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n_similar : int, default=10\n",
    "            Number of most similar items to store for each item\n",
    "        \"\"\"\n",
    "        self.top_n_similar = top_n_similar\n",
    "        self.user_item_matrix = None\n",
    "        self.item_similarity_matrix = None\n",
    "        self.user_mapping = None\n",
    "        self.item_mapping = None\n",
    "        self.reverse_user_mapping = None\n",
    "        self.reverse_item_mapping = None\n",
    "        \n",
    "    def fit(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Build the item-based collaborative filtering model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ratings_df : pandas.DataFrame\n",
    "            DataFrame containing user_id, video_id, and rating columns\n",
    "        \"\"\"\n",
    "        print(\"Starting model fitting...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create mappings between original IDs and matrix indices\n",
    "        self._create_mappings(ratings_df)\n",
    "        \n",
    "        # Build the user-item matrix\n",
    "        self._build_user_item_matrix(ratings_df)\n",
    "        \n",
    "        # Calculate item similarity matrix\n",
    "        self._build_item_similarity_matrix()\n",
    "        \n",
    "        print(f\"Model fitting completed in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "    def _create_mappings(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"Create mappings between original IDs and matrix indices.\"\"\"\n",
    "        print(\"Creating user and item mappings...\")\n",
    "        \n",
    "        # Get unique users and items\n",
    "        unique_users = ratings_df['user_id'].unique()\n",
    "        unique_items = ratings_df['video_id'].unique()\n",
    "        \n",
    "        # Create mappings\n",
    "        self.user_mapping = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        self.item_mapping = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        \n",
    "        # Create reverse mappings (index to original ID)\n",
    "        self.reverse_user_mapping = {idx: user for user, idx in self.user_mapping.items()}\n",
    "        self.reverse_item_mapping = {idx: item for item, idx in self.item_mapping.items()}\n",
    "        \n",
    "        print(f\"Found {len(unique_users)} unique users and {len(unique_items)} unique videos\")\n",
    "        \n",
    "    def _build_user_item_matrix(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"Build the user-item matrix from the ratings DataFrame.\"\"\"\n",
    "        print(\"Building user-item matrix...\")\n",
    "        \n",
    "        # Convert IDs to matrix indices\n",
    "        user_indices = [self.user_mapping[user] for user in ratings_df['user_id']]\n",
    "        item_indices = [self.item_mapping[item] for item in ratings_df['video_id']]\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        n_users = len(self.user_mapping)\n",
    "        n_items = len(self.item_mapping)\n",
    "        \n",
    "        # Convert ratings to float values\n",
    "        ratings = ratings_df['rating'].values.astype(float)\n",
    "        \n",
    "        # Create the sparse matrix\n",
    "        self.user_item_matrix = csr_matrix((ratings, (user_indices, item_indices)), \n",
    "                                          shape=(n_users, n_items))\n",
    "        \n",
    "        print(f\"Created user-item matrix of shape {self.user_item_matrix.shape}\")\n",
    "        \n",
    "    def _build_item_similarity_matrix(self):\n",
    "        \"\"\"Calculate the item-item similarity matrix using cosine similarity.\"\"\"\n",
    "        print(\"Building item similarity matrix...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to item-user matrix (transpose)\n",
    "        item_user_matrix = self.user_item_matrix.T.tocsr()\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        n_items = item_user_matrix.shape[0]\n",
    "        self.item_similarity_matrix = {}\n",
    "        \n",
    "        # For each item, calculate similarity with all other items\n",
    "        for i in range(n_items):\n",
    "            # Print progress every 1000 items\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(f\"Processed {i}/{n_items} items... ({time.time() - start_time:.2f} sec)\")\n",
    "            \n",
    "            # Get the item vector\n",
    "            item_vec = item_user_matrix[i].toarray().flatten()\n",
    "            \n",
    "            # Calculate similarities with all items at once\n",
    "            similarities = cosine_similarity(\n",
    "                item_vec.reshape(1, -1), \n",
    "                item_user_matrix.toarray()\n",
    "            ).flatten()\n",
    "            \n",
    "            # Keep only top N similar items (excluding self)\n",
    "            # First, set self-similarity to -1 to exclude it\n",
    "            similarities[i] = -1\n",
    "            \n",
    "            # Get indices of top N items\n",
    "            top_similar_indices = heapq.nlargest(self.top_n_similar, \n",
    "                                                range(len(similarities)), \n",
    "                                                key=similarities.__getitem__)\n",
    "            \n",
    "            # Store only top N similarities per video\n",
    "            self.item_similarity_matrix[i] = {\n",
    "                sim_idx: similarities[sim_idx] \n",
    "                for sim_idx in top_similar_indices \n",
    "                if similarities[sim_idx] > 0\n",
    "            }\n",
    "        \n",
    "        print(f\"Item similarity matrix built in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    def _wilson_score(self, pos, n, confidence=0.95):\n",
    "        \"\"\"\n",
    "        Calculate the Wilson score interval for a binomial proportion.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pos : int or float\n",
    "            Number of positive ratings or sum of ratings\n",
    "        n : int\n",
    "            Total number of ratings\n",
    "        confidence : float, default=0.95\n",
    "            Confidence level\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Lower bound of Wilson score interval\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        \n",
    "        z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
    "        phat = pos / n\n",
    "        \n",
    "        # Wilson score calculation\n",
    "        score = (phat + z*z/(2*n) - z * math.sqrt((phat*(1-phat) + z*z/(4*n))/n)) / (1 + z*z/n)\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def recommend_popular_items_wilson(self, n_recommendations=10, confidence=0.95, normalize_ratings=True):\n",
    "        \"\"\"\n",
    "        Recommend most popular items based on Wilson score.\n",
    "        Used as fallback for cold-start users.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_recommendations : int, default=10\n",
    "            Number of recommendations to generate\n",
    "        confidence : float, default=0.95\n",
    "            Confidence level for Wilson score\n",
    "        normalize_ratings : bool, default=True\n",
    "            Whether to normalize ratings to 0-1 range\n",
    "                \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, wilson_score) tuples\n",
    "        \"\"\"\n",
    "        # Convert user-item matrix to array for easier processing\n",
    "        matrix = self.user_item_matrix.toarray()\n",
    "        \n",
    "        # Prepare to store results\n",
    "        wilson_scores = []\n",
    "        \n",
    "        # Process each item\n",
    "        for item_idx in range(matrix.shape[1]):\n",
    "            # Get ratings for this item\n",
    "            item_ratings = matrix[:, item_idx]\n",
    "            \n",
    "            # Skip items with no ratings\n",
    "            valid_ratings = item_ratings[item_ratings > 0]\n",
    "            if len(valid_ratings) == 0:\n",
    "                wilson_scores.append(0)\n",
    "                continue\n",
    "            \n",
    "            if normalize_ratings:\n",
    "                # Normalize ratings to 0-1 range\n",
    "                # Assuming ratings are 1-5\n",
    "                norm_ratings = (valid_ratings - 1) / 4\n",
    "                pos = np.sum(norm_ratings)\n",
    "            else:\n",
    "                # Use sum of ratings as \"positive\" outcome\n",
    "                pos = np.sum(valid_ratings)\n",
    "            \n",
    "            # Total number of ratings\n",
    "            n = len(valid_ratings)\n",
    "            \n",
    "            # Calculate Wilson score\n",
    "            score = self._wilson_score(pos, n, confidence)\n",
    "            wilson_scores.append(score)\n",
    "        \n",
    "        # Get indices of top items by Wilson score\n",
    "        top_indices = heapq.nlargest(n_recommendations, \n",
    "                                    range(len(wilson_scores)), \n",
    "                                    key=lambda x: wilson_scores[x])\n",
    "        \n",
    "        # Convert to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[idx], wilson_scores[idx])\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def recommend_for_user(self, user_id: int, n_recommendations=10, exclude_watched=True) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Generate personalized recommendations for a user.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id : int or str\n",
    "            Original user ID\n",
    "        n_recommendations : int, default=10\n",
    "            Number of recommendations to generate\n",
    "        exclude_watched : bool, default=True\n",
    "            Whether to exclude videos the user has already watched\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, predicted_rating) tuples\n",
    "        \"\"\"\n",
    "        # Check if user exists in training data\n",
    "        if user_id not in self.user_mapping:\n",
    "            print(f\"User {user_id} not found in training data. Using popular items instead.\")\n",
    "            return self.recommend_popular_items_wilson(n_recommendations)\n",
    "        \n",
    "        # Get user index\n",
    "        user_idx = self.user_mapping[user_id]\n",
    "        \n",
    "        # Get user's ratings\n",
    "        user_ratings = self.user_item_matrix[user_idx].toarray().flatten()\n",
    "        watched_items = np.where(user_ratings > 0)[0]\n",
    "        \n",
    "        if len(watched_items) == 0:\n",
    "            print(f\"User {user_id} has no ratings. Using popular items instead.\")\n",
    "            return self.recommend_popular_items_wilson(n_recommendations)\n",
    "        \n",
    "        # Initialize recommendation scores\n",
    "        scores = defaultdict(float)\n",
    "        \n",
    "        # For each rated item\n",
    "        for item_idx in watched_items:\n",
    "            # Get user's rating for this item\n",
    "            item_rating = user_ratings[item_idx]\n",
    "            \n",
    "            # Skip low ratings (optional - you might want to consider negative feedback)\n",
    "            if item_rating < 3:\n",
    "                continue\n",
    "                \n",
    "            # Get similar items\n",
    "            if item_idx in self.item_similarity_matrix:\n",
    "                # For each similar item\n",
    "                for similar_item, similarity in self.item_similarity_matrix[item_idx].items():\n",
    "                    # Skip if user has already watched this item and we want to exclude watched\n",
    "                    if exclude_watched and user_ratings[similar_item] > 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Weight by both rating and similarity\n",
    "                    scores[similar_item] += similarity * item_rating\n",
    "        \n",
    "        # If we have no recommendations after filtering\n",
    "        if len(scores) == 0:\n",
    "            return self.recommend_popular_items_wilson(n_recommendations)\n",
    "        \n",
    "        # Sort by score and take top N\n",
    "        top_item_indices = heapq.nlargest(n_recommendations, \n",
    "                                         scores.keys(), \n",
    "                                         key=scores.get)\n",
    "        \n",
    "        # Convert back to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[item_idx], scores[item_idx])\n",
    "            for item_idx in top_item_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def recommend_similar_items(self, video_id, n_recommendations=10) -> List:\n",
    "        \"\"\"\n",
    "        Find videos similar to a given video.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        video_id : int or str\n",
    "            Original video ID\n",
    "        n_recommendations : int, default=10\n",
    "            Number of similar videos to recommend\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        # Check if item exists in training data\n",
    "        if video_id not in self.item_mapping:\n",
    "            print(f\"Video {video_id} not found in training data.\")\n",
    "            return []\n",
    "        \n",
    "        # Get item index\n",
    "        item_idx = self.item_mapping[video_id]\n",
    "        \n",
    "        # If no similarity data for this item\n",
    "        if item_idx not in self.item_similarity_matrix:\n",
    "            print(f\"No similarity data for video {video_id}.\")\n",
    "            return []\n",
    "        \n",
    "        # Get similar items\n",
    "        similar_items = self.item_similarity_matrix[item_idx]\n",
    "        \n",
    "        # Sort by similarity and take top N\n",
    "        top_similar = heapq.nlargest(n_recommendations, \n",
    "                                    similar_items.keys(), \n",
    "                                    key=similar_items.get)\n",
    "        \n",
    "        # Convert back to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[sim_idx], similar_items[sim_idx])\n",
    "            for sim_idx in top_similar\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the model to a file.\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        model_data = {\n",
    "            'user_mapping': self.user_mapping,\n",
    "            'item_mapping': self.item_mapping,\n",
    "            'reverse_user_mapping': self.reverse_user_mapping,\n",
    "            'reverse_item_mapping': self.reverse_item_mapping,\n",
    "            'user_item_matrix': self.user_item_matrix,\n",
    "            'item_similarity_matrix': self.item_similarity_matrix,\n",
    "            'top_n_similar': self.top_n_similar\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        \"\"\"Load a saved model from a file.\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Create new instance\n",
    "        model = cls(top_n_similar=model_data['top_n_similar'])\n",
    "        \n",
    "        # Restore attributes\n",
    "        model.user_mapping = model_data['user_mapping']\n",
    "        model.item_mapping = model_data['item_mapping']\n",
    "        model.reverse_user_mapping = model_data['reverse_user_mapping']\n",
    "        model.reverse_item_mapping = model_data['reverse_item_mapping']\n",
    "        model.user_item_matrix = model_data['user_item_matrix']\n",
    "        model.item_similarity_matrix = model_data['item_similarity_matrix']\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Example for loading data from CSV file and full pipeline\n",
    "def full_pipeline_example():\n",
    "    \"\"\"\n",
    "    Example showing how to use the ItemBasedCF class with a CSV file.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = video_rating_df\n",
    "    \n",
    "    # Data exploration and preprocessing\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Number of unique users: {df['user_id'].nunique()}\")\n",
    "    print(f\"Number of unique videos: {df['video_id'].nunique()}\")\n",
    "    print(f\"Rating distribution:\\n{df['rating'].value_counts(normalize=True)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"Warning: Dataset contains missing values\")\n",
    "        df = df.dropna()\n",
    "    \n",
    "    # Split into train (80%) and test (20%)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model = ItemBasedCF(top_n_similar=20)\n",
    "    model.fit(train_df)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_model('item_cf_model.pkl')\n",
    "    \n",
    "    # Evaluate the model\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import math\n",
    "    \n",
    "    # Get test user-item pairs\n",
    "    test_users = test_df['user_id'].unique()\n",
    "    \n",
    "    # Calculate RMSE for test set\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        # Get user's test items\n",
    "        user_test_items = test_df[test_df['user_id'] == user_id]\n",
    "        \n",
    "        if user_id not in model.user_mapping:\n",
    "            continue\n",
    "        \n",
    "        user_idx = model.user_mapping[user_id]\n",
    "        user_ratings = model.user_item_matrix[user_idx].toarray().flatten()\n",
    "        watched_items = np.where(user_ratings > 0)[0]\n",
    "        \n",
    "        for _, row in user_test_items.iterrows():\n",
    "            if row['video_id'] not in model.item_mapping:\n",
    "                continue\n",
    "                \n",
    "            item_idx = model.item_mapping[row['video_id']]\n",
    "            actual_rating = row['rating']\n",
    "            \n",
    "            # Predict rating using item-based CF\n",
    "            predicted_rating = 0\n",
    "            total_similarity = 0\n",
    "            \n",
    "            for watched_item in watched_items:\n",
    "                # Skip the current test item\n",
    "                if watched_item == item_idx:\n",
    "                    continue\n",
    "                    \n",
    "                # Get user's rating for this item\n",
    "                rating = user_ratings[watched_item]\n",
    "                \n",
    "                # Get similarity between this item and test item\n",
    "                if watched_item in model.item_similarity_matrix and item_idx in model.item_similarity_matrix[watched_item]:\n",
    "                    similarity = model.item_similarity_matrix[watched_item][item_idx]\n",
    "                    predicted_rating += similarity * rating\n",
    "                    total_similarity += similarity\n",
    "            \n",
    "            # Normalize by total similarity\n",
    "            if total_similarity > 0:\n",
    "                predicted_rating /= total_similarity\n",
    "                \n",
    "                actual_ratings.append(actual_rating)\n",
    "                predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = math.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "    print(f\"Test RMSE: {rmse}\")\n",
    "    \n",
    "    # Generate recommendations for a sample user\n",
    "    sample_user = df['user_id'].iloc[0]\n",
    "    recommendations = model.recommend_for_user(sample_user, n_recommendations=5)\n",
    "    print(f\"Top 5 recommendations for user {sample_user}:\")\n",
    "    for video_id, score in recommendations:\n",
    "        print(f\"  Video {video_id}: Score {score:.4f}\")\n",
    "\n",
    "# Uncomment to run the full pipeline example\n",
    "full_pipeline_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations for a sample user\n",
    "sample_user = df['user_id'].iloc[0]\n",
    "recommendations = model.recommend_for_user(sample_user, n_recommendations=5)\n",
    "print(f\"Top 5 recommendations for user {sample_user}:\")\n",
    "for video_id, score in recommendations:\n",
    "    print(f\"  Video {video_id}: Score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-to-Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "class UserBasedCF:\n",
    "    def __init__(self, n_neighbors=20, min_neighbors=1):\n",
    "        \"\"\"\n",
    "        Initialize User-Based Collaborative Filtering model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_neighbors : int, default=20\n",
    "            Number of neighbors to use for prediction\n",
    "        min_neighbors : int, default=1\n",
    "            Minimum number of neighbors needed to make a prediction\n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_neighbors = min_neighbors\n",
    "        self.user_similarity_matrix = None\n",
    "        self.ratings_matrix = None\n",
    "        self.user_ids = None\n",
    "        self.item_ids = None\n",
    "        self.user_means = None\n",
    "        \n",
    "    def fit(self, ratings_df):\n",
    "        \"\"\"\n",
    "        Fit the model with training data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ratings_df : pandas DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "            The training ratings data\n",
    "        \"\"\"\n",
    "        # Create user-item matrix\n",
    "        \n",
    "        self.ratings_matrix = ratings_df.pivot_table(\n",
    "            index='user_id', \n",
    "            columns='item_id', \n",
    "            values='rating'\n",
    "        ).fillna(0)\n",
    "      \n",
    "        \n",
    "        \n",
    "        self.user_ids = self.ratings_matrix.index.tolist()\n",
    "        self.item_ids = self.ratings_matrix.columns.tolist()\n",
    "        \n",
    "        # Calculate mean rating for each user for later use in prediction\n",
    "        self.user_means = self.ratings_matrix.mean(axis=1)\n",
    "        \n",
    "        # Normalize ratings by subtracting user means\n",
    "        normalized_ratings = self.ratings_matrix.subtract(self.user_means, axis=0)\n",
    "        \n",
    "        # Calculate user similarity matrix using cosine similarity\n",
    "        self.user_similarity_matrix = cosine_similarity(normalized_ratings)\n",
    "        self.user_similarity_matrix = pd.DataFrame(\n",
    "            self.user_similarity_matrix,\n",
    "            index=self.user_ids,\n",
    "            columns=self.user_ids\n",
    "        )\n",
    "        \n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        Predict rating for a single user-item pair\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id : int or str\n",
    "            The user ID\n",
    "        item_id : int or str\n",
    "            The item ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Predicted rating\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_ids:\n",
    "            return self.ratings_matrix.mean().mean()  # Global mean for new users\n",
    "        \n",
    "        if item_id not in self.item_ids:\n",
    "            return self.user_means[user_id]  # User mean for new items\n",
    "        \n",
    "        # Get user's row index\n",
    "        user_idx = self.user_ids.index(user_id)\n",
    "        \n",
    "        # Get all users' similarities to target user\n",
    "        similarities = self.user_similarity_matrix.iloc[user_idx].values\n",
    "        \n",
    "        # Get all users' ratings for the target item\n",
    "        ratings = self.ratings_matrix[item_id].values\n",
    "        \n",
    "        # Mask when ratings are zero (unrated)\n",
    "        mask = ratings != 0\n",
    "        \n",
    "        # Only consider similar users who have rated the item\n",
    "        sims = similarities[mask]\n",
    "        rs = ratings[mask]\n",
    "        \n",
    "        # Check if we have enough neighbors\n",
    "        if len(sims) < self.min_neighbors:\n",
    "            return self.user_means[user_id]\n",
    "        \n",
    "        # Sort by similarity and take top k neighbors\n",
    "        if len(sims) > self.n_neighbors:\n",
    "            idx = np.argsort(sims)[-self.n_neighbors:]\n",
    "            sims = sims[idx]\n",
    "            rs = rs[idx]\n",
    "        \n",
    "        # Normalize ratings by subtracting user means\n",
    "        user_means_array = np.array([self.user_means[uid] for uid in np.array(self.user_ids)[mask]])\n",
    "        if len(user_means_array) > self.n_neighbors:\n",
    "            user_means_array = user_means_array[idx]\n",
    "        \n",
    "        rs_norm = rs - user_means_array\n",
    "        \n",
    "        # Calculate weighted average\n",
    "        if np.sum(np.abs(sims)) > 0:\n",
    "            pred = self.user_means[user_id] + np.sum(sims * rs_norm) / np.sum(np.abs(sims))\n",
    "        else:\n",
    "            pred = self.user_means[user_id]\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def recommend(self, user_id, n_items=10, exclude_rated=True):\n",
    "        \"\"\"\n",
    "        Generate recommendations for a user\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id : int or str\n",
    "            The user ID\n",
    "        n_items : int, default=10\n",
    "            Number of items to recommend\n",
    "        exclude_rated : bool, default=True\n",
    "            Whether to exclude already rated items\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list : List of recommended item IDs\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_ids:\n",
    "            # For new users, recommend most popular items\n",
    "            item_popularity = self.ratings_matrix.sum().sort_values(ascending=False)\n",
    "            return item_popularity.index[:n_items].tolist()\n",
    "        \n",
    "        # Get items the user has already rated\n",
    "        rated_items = self.ratings_matrix.loc[user_id]\n",
    "        rated_items = rated_items[rated_items > 0].index.tolist() if exclude_rated else []\n",
    "        \n",
    "        # Calculate predicted ratings for all unrated items\n",
    "        all_items = [item for item in self.item_ids if item not in rated_items]\n",
    "        predicted_ratings = {item: self.predict(user_id, item) for item in all_items}\n",
    "        \n",
    "        # Sort items by predicted rating\n",
    "        sorted_items = sorted(predicted_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top N recommendations\n",
    "        return [item for item, rating in sorted_items[:n_items]]\n",
    "    \n",
    "    def evaluate(self, test_df):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_df : pandas DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "            The test ratings data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Root Mean Squared Error (RMSE)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        for _, row in test_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            item_id = row['item_id']\n",
    "            \n",
    "            if user_id in self.user_ids and item_id in self.item_ids:\n",
    "                pred = self.predict(user_id, item_id)\n",
    "                predictions.append(pred)\n",
    "                actuals.append(row['rating'])\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = sqrt(mean_squared_error(actuals, predictions))\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2594</td>\n",
       "      <td>1889</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2032</td>\n",
       "      <td>1908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979</td>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>1951</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2156</td>\n",
       "      <td>485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9295</th>\n",
       "      <td>2342</td>\n",
       "      <td>2286</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9296</th>\n",
       "      <td>631</td>\n",
       "      <td>2286</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9297</th>\n",
       "      <td>1746</td>\n",
       "      <td>721</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9298</th>\n",
       "      <td>1746</td>\n",
       "      <td>844</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9299</th>\n",
       "      <td>3798</td>\n",
       "      <td>470</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9271 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id  rating\n",
       "0        2594     1889       5\n",
       "1        2032     1908       1\n",
       "2        1979      448       5\n",
       "3          71     1951       5\n",
       "4        2156      485       1\n",
       "...       ...      ...     ...\n",
       "9295     2342     2286       5\n",
       "9296      631     2286       5\n",
       "9297     1746      721       5\n",
       "9298     1746      844       5\n",
       "9299     3798      470       4\n",
       "\n",
       "[9271 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2i_cf_df = cf_df_encoded[[\"user_idx\", \"item_idx\", \"rating\"]]\n",
    "u2i_cf_df = u2i_cf_df.rename(columns={\"user_idx\": \"user_id\", \"item_idx\":\"item_id\"})\n",
    "u2i_cf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1645/3596032178.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  u2i_cf_df.rename(columns={\"user_idx\": \"user_id\", \"item_idx\":\"item_id\"}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 1: [322, 1160, 1625, 1174, 1341]\n",
      "RMSE: 4.915260353628598\n"
     ]
    }
   ],
   "source": [
    "u2i_cf_df = cf_df_encoded[[\"user_idx\", \"item_idx\", \"rating\"]]\n",
    "u2i_cf_df.rename(columns={\"user_idx\": \"user_id\", \"item_idx\":\"item_id\"}, inplace=True)\n",
    "\n",
    "# Split into train and test (80:20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(u2i_cf_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train model\n",
    "cf_model = UserBasedCF(n_neighbors=2)\n",
    "cf_model.fit(train_df)\n",
    "\n",
    "# Make recommendations for a user\n",
    "recommendations = cf_model.recommend(user_id=1, n_items=5)\n",
    "print(f\"Recommendations for user 1: {recommendations}\")\n",
    "\n",
    "# Evaluate model\n",
    "rmse = cf_model.evaluate(test_df)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>item_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>DNHJJnbYyXd31WUkq0GPXZkkFqr1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>835</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>zNWhroyJfITeNCa9UDijTTvgks63</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3847</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>cpxKW5kyPihjXGPsptgecvot9SE2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>2453</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>yDvMmF1T7NfNKpYXsQms5HkdHFt1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3783</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Ku0YGfIHZuMtSHDmq9yCtEe0DkB3</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1285</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>LhavCaMxdPexiv7AEHdQQGmDU6s1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1344</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>XpVUrvD6WhOEeblPxAlg6wzd5Z92</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>2107</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>SgS7wLNOMoQhWBMx8sQksHWYGdF3</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1798</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>HVgi0BgIVATRrGo66Hi8symnKGy2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1094</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>E7vSNnez82OlQlKEuYumBiyTQ4o2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>878</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>L43Y0iD0KgXyCR0q5kN6u7o5Li33</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1300</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>ib97653dtnXgqeVU2NyiTXu57ap2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>1</td>\n",
       "      <td>2782</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>S5I2AU99q3PnKCEDZFObhZVMjtn1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1760</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>xnJtfvYje0cCY2xeHZFUV1gtBvA3</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3757</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>cIoMpm9xSgbO7Kjy5wdn0zGwkFN2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>2414</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>tjeKvSHYf9NEwIctzH0qHDB5Ozw1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3495</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>8TYdO5PII2W2J4Gk0PDqgzDQjfL2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>524</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>KCv9lvF5AmOYVrFZpGovyzbaPk53</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1246</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>vsT95uFMBON7lXzhNQCLetqXzTi1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3634</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>GprKrVkpDPUEqM0fah5rjV8qoxn1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1054</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>ZQfXpqhdgQVGFAVsAdILDduDjD23</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>2221</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>LhavCaMxdPexiv7AEHdQQGmDU6s1</td>\n",
       "      <td>96499535-2507-427d-bef6-807a05a3535c</td>\n",
       "      <td>5</td>\n",
       "      <td>1344</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>f0gbuiX99XdyDpCW97yZIqwmchH2</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>2573</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>KlneNshPSnONSvlsxgBcBRqhKZG3</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>1278</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>DmzvkB3dQzL7p58SoPhn8RV3Q8x1</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>855</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>H4J1Wy2pPsT6n7vhblSqGUseStt2</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>1068</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6156</th>\n",
       "      <td>kY8OADiRYigje5W2u0fsVfXFNkT2</td>\n",
       "      <td>22da7b5c-8fa4-47e9-84bb-46480cd3e5e6</td>\n",
       "      <td>5</td>\n",
       "      <td>2896</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6244</th>\n",
       "      <td>5JcJDaJXojfFYVaqN6HdRkM1yts1</td>\n",
       "      <td>82e78c44-46f3-462b-a795-ba30e98be9ae</td>\n",
       "      <td>5</td>\n",
       "      <td>352</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td>FqgWj0NlZJdUfF1pAr3KRWmcckx1</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>990</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540</th>\n",
       "      <td>s4UT1Z7mS4ezfHdmMKwcPGVqg0T2</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>4</td>\n",
       "      <td>3383</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>nOZhb970cDOy7ZFAAthuUxytPbR2</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>3048</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           user_id                              video_id  \\\n",
       "78    DNHJJnbYyXd31WUkq0GPXZkkFqr1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "188   zNWhroyJfITeNCa9UDijTTvgks63  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "210   cpxKW5kyPihjXGPsptgecvot9SE2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "373   yDvMmF1T7NfNKpYXsQms5HkdHFt1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "410   Ku0YGfIHZuMtSHDmq9yCtEe0DkB3  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "459   LhavCaMxdPexiv7AEHdQQGmDU6s1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "461   XpVUrvD6WhOEeblPxAlg6wzd5Z92  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "476   SgS7wLNOMoQhWBMx8sQksHWYGdF3  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "539   HVgi0BgIVATRrGo66Hi8symnKGy2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "581   E7vSNnez82OlQlKEuYumBiyTQ4o2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "593   L43Y0iD0KgXyCR0q5kN6u7o5Li33  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "635   ib97653dtnXgqeVU2NyiTXu57ap2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "647   S5I2AU99q3PnKCEDZFObhZVMjtn1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "685   xnJtfvYje0cCY2xeHZFUV1gtBvA3  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "691   cIoMpm9xSgbO7Kjy5wdn0zGwkFN2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "713   tjeKvSHYf9NEwIctzH0qHDB5Ozw1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "715   8TYdO5PII2W2J4Gk0PDqgzDQjfL2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "716   KCv9lvF5AmOYVrFZpGovyzbaPk53  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "722   vsT95uFMBON7lXzhNQCLetqXzTi1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "729   GprKrVkpDPUEqM0fah5rjV8qoxn1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "731   ZQfXpqhdgQVGFAVsAdILDduDjD23  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "1622  LhavCaMxdPexiv7AEHdQQGmDU6s1  96499535-2507-427d-bef6-807a05a3535c   \n",
       "2324  f0gbuiX99XdyDpCW97yZIqwmchH2  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "3752  KlneNshPSnONSvlsxgBcBRqhKZG3  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "5635  DmzvkB3dQzL7p58SoPhn8RV3Q8x1  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "5910  H4J1Wy2pPsT6n7vhblSqGUseStt2  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "6156  kY8OADiRYigje5W2u0fsVfXFNkT2  22da7b5c-8fa4-47e9-84bb-46480cd3e5e6   \n",
       "6244  5JcJDaJXojfFYVaqN6HdRkM1yts1  82e78c44-46f3-462b-a795-ba30e98be9ae   \n",
       "6490  FqgWj0NlZJdUfF1pAr3KRWmcckx1  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "6540  s4UT1Z7mS4ezfHdmMKwcPGVqg0T2  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "7270  nOZhb970cDOy7ZFAAthuUxytPbR2  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "\n",
       "      rating  user_idx  item_idx  \n",
       "78         5       835      1174  \n",
       "188        5      3847      1174  \n",
       "210        5      2453      1174  \n",
       "373        5      3783      1174  \n",
       "410        5      1285      1174  \n",
       "459        5      1344      1174  \n",
       "461        5      2107      1174  \n",
       "476        5      1798      1174  \n",
       "539        5      1094      1174  \n",
       "581        5       878      1174  \n",
       "593        5      1300      1174  \n",
       "635        1      2782      1174  \n",
       "647        5      1760      1174  \n",
       "685        5      3757      1174  \n",
       "691        5      2414      1174  \n",
       "713        5      3495      1174  \n",
       "715        5       524      1174  \n",
       "716        5      1246      1174  \n",
       "722        5      3634      1174  \n",
       "729        5      1054      1174  \n",
       "731        5      2221      1174  \n",
       "1622       5      1344      1341  \n",
       "2324       5      2573      1625  \n",
       "3752       5      1278      1625  \n",
       "5635       5       855      1625  \n",
       "5910       5      1068      1625  \n",
       "6156       5      2896       322  \n",
       "6244       5       352      1160  \n",
       "6490       5       990      1625  \n",
       "6540       4      3383      1625  \n",
       "7270       5      3048      1625  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_df_encoded[cf_df_encoded[\"item_idx\"].isin([322, 1160, 1625, 1174, 1341])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import time\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class ItemBasedCFRecommender:\n",
    "    \"\"\"\n",
    "    Item-based Collaborative Filtering recommendation system\n",
    "    specifically designed for video recommendations with user ratings.\n",
    "    Enhanced with caching and scalability improvements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, top_n_similar=10, cache_size=1024, similarity_threshold=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the recommender system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n_similar : int, default=10\n",
    "            Number of most similar items to store for each item\n",
    "        cache_size : int, default=1024\n",
    "            Size of the LRU cache for recommendation results\n",
    "        similarity_threshold : float, default=0.0\n",
    "            Minimum similarity threshold for considering items related\n",
    "        \"\"\"\n",
    "        self.top_n_similar = top_n_similar\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.user_item_matrix = None\n",
    "        self.item_similarity_matrix = None\n",
    "        self.user_mapping = None\n",
    "        self.item_mapping = None\n",
    "        self.reverse_user_mapping = None\n",
    "        self.reverse_item_mapping = None\n",
    "        self.popular_items_cache = None\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        # Configure the LRU cache decorators\n",
    "        self._configure_caches()\n",
    "        \n",
    "    def _configure_caches(self):\n",
    "        \"\"\"Configure LRU caches with the specified size\"\"\"\n",
    "        # Make the recommend_for_user method use LRU cache\n",
    "        self.recommend_for_user = lru_cache(maxsize=self.cache_size)(self._recommend_for_user)\n",
    "        # Make the recommend_similar_items method use LRU cache\n",
    "        self.recommend_similar_items = lru_cache(maxsize=self.cache_size)(self._recommend_similar_items)\n",
    "        \n",
    "    def fit(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Build the item-based collaborative filtering model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ratings_df : pandas.DataFrame\n",
    "            DataFrame containing user_id, video_id, and rating columns\n",
    "        \"\"\"\n",
    "        print(\"Starting model fitting...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create mappings between original IDs and matrix indices\n",
    "        self._create_mappings(ratings_df)\n",
    "        \n",
    "        # Build the user-item matrix\n",
    "        self._build_user_item_matrix(ratings_df)\n",
    "        \n",
    "        # Calculate item similarity matrix\n",
    "        self._build_item_similarity_matrix()\n",
    "        \n",
    "        # Pre-compute popular items to use as fallback\n",
    "        self.popular_items_cache = self.recommend_popular_items_wilson()\n",
    "        \n",
    "        print(f\"Model fitting completed in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "    def _create_mappings(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"Create mappings between original IDs and matrix indices.\"\"\"\n",
    "        print(\"Creating user and item mappings...\")\n",
    "        \n",
    "        # Get unique users and items\n",
    "        unique_users = ratings_df['user_id'].unique()\n",
    "        unique_items = ratings_df['video_id'].unique()\n",
    "        \n",
    "        # Create mappings\n",
    "        self.user_mapping = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        self.item_mapping = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        \n",
    "        # Create reverse mappings (index to original ID)\n",
    "        self.reverse_user_mapping = {idx: user for user, idx in self.user_mapping.items()}\n",
    "        self.reverse_item_mapping = {idx: item for item, idx in self.item_mapping.items()}\n",
    "        \n",
    "        print(f\"Found {len(unique_users)} unique users and {len(unique_items)} unique videos\")\n",
    "        \n",
    "    def _build_user_item_matrix(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"Build the user-item matrix from the ratings DataFrame.\"\"\"\n",
    "        print(\"Building user-item matrix...\")\n",
    "        \n",
    "        # Convert IDs to matrix indices\n",
    "        user_indices = [self.user_mapping[user] for user in ratings_df['user_id']]\n",
    "        item_indices = [self.item_mapping[item] for item in ratings_df['video_id']]\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        n_users = len(self.user_mapping)\n",
    "        n_items = len(self.item_mapping)\n",
    "        \n",
    "        # Convert ratings to float values\n",
    "        ratings = ratings_df['rating'].values.astype(float)\n",
    "        \n",
    "        # Create the sparse matrix\n",
    "        self.user_item_matrix = csr_matrix((ratings, (user_indices, item_indices)), \n",
    "                                          shape=(n_users, n_items))\n",
    "        \n",
    "        print(f\"Created user-item matrix of shape {self.user_item_matrix.shape}\")\n",
    "        \n",
    "    def _build_item_similarity_matrix(self):\n",
    "        \"\"\"Calculate the item-item similarity matrix using cosine similarity.\"\"\"\n",
    "        print(\"Building item similarity matrix...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to item-user matrix (transpose)\n",
    "        item_user_matrix = self.user_item_matrix.T.tocsr()\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        n_items = item_user_matrix.shape[0]\n",
    "        self.item_similarity_matrix = {}\n",
    "        \n",
    "        # Process items in batches to reduce memory usage\n",
    "        batch_size = 1000  # Adjust based on available memory\n",
    "        \n",
    "        for batch_start in range(0, n_items, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, n_items)\n",
    "            if batch_start > 0:\n",
    "                print(f\"Processing batch {batch_start//batch_size + 1}/{(n_items + batch_size - 1)//batch_size}...\")\n",
    "            \n",
    "            # Extract the batch of item vectors\n",
    "            batch_vectors = item_user_matrix[batch_start:batch_end].toarray()\n",
    "            \n",
    "            # Calculate similarities with all items at once for this batch\n",
    "            similarities = cosine_similarity(batch_vectors, item_user_matrix.toarray())\n",
    "            \n",
    "            # Process each item in the batch\n",
    "            for i, item_idx in enumerate(range(batch_start, batch_end)):\n",
    "                # Set self-similarity to -1 to exclude it\n",
    "                similarities[i, item_idx] = -1\n",
    "                \n",
    "                # Filter by threshold before finding top N\n",
    "                valid_indices = np.where(similarities[i] > self.similarity_threshold)[0]\n",
    "                \n",
    "                # Get top N similar items\n",
    "                if len(valid_indices) > self.top_n_similar:\n",
    "                    # Use argpartition for efficiency (O(n) instead of O(n log n) for full sort)\n",
    "                    top_indices = np.argpartition(similarities[i, valid_indices], -self.top_n_similar)[-self.top_n_similar:]\n",
    "                    top_similar_indices = valid_indices[top_indices]\n",
    "                else:\n",
    "                    top_similar_indices = valid_indices\n",
    "                \n",
    "                # Store only nonzero similarities in a dictionary (sparse representation)\n",
    "                self.item_similarity_matrix[item_idx] = {\n",
    "                    sim_idx: similarities[i, sim_idx] \n",
    "                    for sim_idx in top_similar_indices \n",
    "                    if similarities[i, sim_idx] > 0\n",
    "                }\n",
    "        \n",
    "        print(f\"Item similarity matrix built in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Calculate memory usage\n",
    "        similarity_size = sum(len(similarities) for similarities in self.item_similarity_matrix.values())\n",
    "        print(f\"Item similarity matrix contains {similarity_size} nonzero elements\")\n",
    "\n",
    "    def _wilson_score(self, pos, n, confidence=0.95):\n",
    "        \"\"\"\n",
    "        Calculate the Wilson score interval for a binomial proportion.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pos : int or float\n",
    "            Number of positive ratings or sum of ratings\n",
    "        n : int\n",
    "            Total number of ratings\n",
    "        confidence : float, default=0.95\n",
    "            Confidence level\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Lower bound of Wilson score interval\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Handle case where pos > n (which can happen when using sum of ratings)\n",
    "        if pos > n:\n",
    "            # For Wilson score calculation, proportion must be between 0 and 1\n",
    "            # Normalize the positive score to be within valid range\n",
    "            phat = 1.0\n",
    "        else:\n",
    "            phat = pos / n\n",
    "        \n",
    "        z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
    "        \n",
    "        # Ensure phat is in the valid range [0, 1]\n",
    "        phat = min(max(phat, 0), 1)\n",
    "        \n",
    "        # Wilson score calculation\n",
    "        score = (phat + z*z/(2*n) - z * math.sqrt((phat*(1-phat) + z*z/(4*n))/n)) / (1 + z*z/n)\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def recommend_popular_items_wilson(self, n_recommendations=10, confidence=0.95, normalize_ratings=True):\n",
    "        \"\"\"\n",
    "        Recommend most popular items based on Wilson score.\n",
    "        Used as fallback for cold-start users.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_recommendations : int, default=10\n",
    "            Number of recommendations to generate\n",
    "        confidence : float, default=0.95\n",
    "            Confidence level for Wilson score\n",
    "        normalize_ratings : bool, default=True\n",
    "            Whether to normalize ratings to 0-1 range\n",
    "                \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, wilson_score) tuples\n",
    "        \"\"\"\n",
    "        # Convert user-item matrix to array for easier processing\n",
    "        # Use CSR matrix methods to avoid full conversion to dense array\n",
    "        ratings_sum = np.array(self.user_item_matrix.sum(axis=0))[0]\n",
    "        \n",
    "        # Count nonzero elements per column (number of ratings per item)\n",
    "        ratings_count = np.array(self.user_item_matrix.getnnz(axis=0))\n",
    "        \n",
    "        # Prepare to store results\n",
    "        wilson_scores = []\n",
    "        \n",
    "        # Process each item\n",
    "        for item_idx in range(self.user_item_matrix.shape[1]):\n",
    "            # Skip items with no ratings\n",
    "            if ratings_count[item_idx] == 0:\n",
    "                wilson_scores.append(0)\n",
    "                continue\n",
    "            \n",
    "            if normalize_ratings:\n",
    "                # For normalized ratings, we need to fetch the actual ratings\n",
    "                # Get the column for this item\n",
    "                col = self.user_item_matrix.getcol(item_idx)\n",
    "                # Extract nonzero values\n",
    "                ratings = col.data\n",
    "                \n",
    "                # Normalize ratings to 0-1 range (assuming 1-5 scale)\n",
    "                # Ensure we handle ratings outside the expected range\n",
    "                min_rating = 1  # Minimum expected rating\n",
    "                max_rating = 5  # Maximum expected rating\n",
    "                norm_ratings = np.clip((ratings - min_rating) / (max_rating - min_rating), 0, 1)\n",
    "                pos = np.sum(norm_ratings)\n",
    "                # Use number of ratings as n for normalized case\n",
    "                n = len(ratings)\n",
    "            else:\n",
    "                # Use sum of ratings as \"positive\" outcome\n",
    "                pos = ratings_sum[item_idx]\n",
    "                # For non-normalized case, we need a reasonable denominator\n",
    "                # We'll use the maximum possible rating sum (n * max_rating)\n",
    "                n = ratings_count[item_idx] * 5  # Assuming 5 is the max rating\n",
    "            \n",
    "            # Calculate Wilson score\n",
    "            score = self._wilson_score(pos, n, confidence)\n",
    "            wilson_scores.append(score)\n",
    "        \n",
    "        # Use numpy for efficient top-N selection\n",
    "        if n_recommendations >= len(wilson_scores):\n",
    "            top_indices = np.argsort(wilson_scores)[::-1]\n",
    "        else:\n",
    "            # Use argpartition for more efficient selection\n",
    "            top_indices = np.argpartition(wilson_scores, -n_recommendations)[-n_recommendations:]\n",
    "            # Sort the top N\n",
    "            top_indices = top_indices[np.argsort([-wilson_scores[i] for i in top_indices])]\n",
    "        \n",
    "        # Convert to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[idx], wilson_scores[idx])\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _recommend_for_user(self, user_id, n_recommendations=10, exclude_watched=True) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Generate personalized recommendations for a user.\n",
    "        This internal method does the actual work and is wrapped by the cached public method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id : int or str\n",
    "            Original user ID\n",
    "        n_recommendations : int, default=10\n",
    "            Number of recommendations to generate\n",
    "        exclude_watched : bool, default=True\n",
    "            Whether to exclude videos the user has already watched\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, predicted_rating) tuples\n",
    "        \"\"\"\n",
    "        # Check if user exists in training data\n",
    "        if user_id not in self.user_mapping:\n",
    "            print(f\"User {user_id} not found in training data. Using popular items instead.\")\n",
    "            return self.popular_items_cache[:n_recommendations]\n",
    "        \n",
    "        # Get user index\n",
    "        user_idx = self.user_mapping[user_id]\n",
    "        \n",
    "        # Get user's ratings efficiently from sparse matrix\n",
    "        user_vector = self.user_item_matrix[user_idx]\n",
    "        watched_items = user_vector.indices\n",
    "        watched_ratings = user_vector.data\n",
    "        \n",
    "        if len(watched_items) == 0:\n",
    "            print(f\"User {user_id} has no ratings. Using popular items instead.\")\n",
    "            return self.popular_items_cache[:n_recommendations]\n",
    "        \n",
    "        # Initialize recommendation scores as sparse dictionary for efficiency\n",
    "        scores = defaultdict(float)\n",
    "        total_similarity = defaultdict(float)\n",
    "        \n",
    "        # For each rated item\n",
    "        for idx, item_idx in enumerate(watched_items):\n",
    "            # Get user's rating for this item\n",
    "            item_rating = watched_ratings[idx]\n",
    "            \n",
    "            # Skip low ratings (optional - you might want to consider negative feedback)\n",
    "            if item_rating < 3:\n",
    "                continue\n",
    "                \n",
    "            # Get similar items\n",
    "            if item_idx in self.item_similarity_matrix:\n",
    "                # For each similar item\n",
    "                for similar_item, similarity in self.item_similarity_matrix[item_idx].items():\n",
    "                    # Skip if user has already watched this item and we want to exclude watched\n",
    "                    if exclude_watched and similar_item in watched_items:\n",
    "                        continue\n",
    "                    \n",
    "                    # Weight by both rating and similarity\n",
    "                    scores[similar_item] += similarity * item_rating\n",
    "                    # Track total similarity for normalization\n",
    "                    total_similarity[similar_item] += similarity\n",
    "        \n",
    "        # Normalize scores by total similarity for more stable predictions\n",
    "        normalized_scores = {\n",
    "            item_idx: score/total_similarity[item_idx] if total_similarity[item_idx] > 0 else score\n",
    "            for item_idx, score in scores.items()\n",
    "        }\n",
    "        \n",
    "        # If we have no recommendations after filtering\n",
    "        if len(normalized_scores) == 0:\n",
    "            return self.popular_items_cache[:n_recommendations]\n",
    "        \n",
    "        # Sort by score and take top N\n",
    "        top_item_indices = heapq.nlargest(n_recommendations, \n",
    "                                         normalized_scores.keys(), \n",
    "                                         key=normalized_scores.get)\n",
    "        \n",
    "        # Convert back to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[item_idx], normalized_scores[item_idx])\n",
    "            for item_idx in top_item_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _recommend_similar_items(self, video_id, n_recommendations=10) -> List:\n",
    "        \"\"\"\n",
    "        Find videos similar to a given video.\n",
    "        This internal method does the actual work and is wrapped by the cached public method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        video_id : int or str\n",
    "            Original video ID\n",
    "        n_recommendations : int, default=10\n",
    "            Number of similar videos to recommend\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        # Check if item exists in training data\n",
    "        if video_id not in self.item_mapping:\n",
    "            print(f\"Video {video_id} not found in training data.\")\n",
    "            return []\n",
    "        \n",
    "        # Get item index\n",
    "        item_idx = self.item_mapping[video_id]\n",
    "        \n",
    "        # If no similarity data for this item\n",
    "        if item_idx not in self.item_similarity_matrix:\n",
    "            print(f\"No similarity data for video {video_id}.\")\n",
    "            return []\n",
    "        \n",
    "        # Get similar items\n",
    "        similar_items = self.item_similarity_matrix[item_idx]\n",
    "        \n",
    "        # Sort by similarity and take top N\n",
    "        top_similar = heapq.nlargest(n_recommendations, \n",
    "                                    similar_items.keys(), \n",
    "                                    key=similar_items.get)\n",
    "        \n",
    "        # Convert back to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[sim_idx], similar_items[sim_idx])\n",
    "            for sim_idx in top_similar\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def clear_caches(self):\n",
    "        \"\"\"Clear all LRU caches to free memory or refresh recommendations\"\"\"\n",
    "        self.recommend_for_user.cache_clear()\n",
    "        self.recommend_similar_items.cache_clear()\n",
    "    \n",
    "    def save_model(self, filepath: str, model_file_name:str):\n",
    "        \"\"\"Save the model to disk\"\"\"\n",
    "        \n",
    "        # Create a dictionary with the model data\n",
    "        model_data = {\n",
    "            'user_mapping': self.user_mapping,\n",
    "            'item_mapping': self.item_mapping,\n",
    "            'reverse_user_mapping': self.reverse_user_mapping,\n",
    "            'reverse_item_mapping': self.reverse_item_mapping,\n",
    "            'item_similarity_matrix': self.item_similarity_matrix,\n",
    "            'popular_items_cache': self.popular_items_cache,\n",
    "            'top_n_similar': self.top_n_similar,\n",
    "            'similarity_threshold': self.similarity_threshold,\n",
    "            'cache_size': self.cache_size\n",
    "        }\n",
    "        \n",
    "        # Save sparse matrix separately for efficiency\n",
    "        if self.user_item_matrix is not None:\n",
    "            from scipy import sparse\n",
    "            sparse.save_npz(f\"{filepath}/{model_file_name}_user_item_matrix.npz\", self.user_item_matrix)\n",
    "            \n",
    "        # Save the rest of the data\n",
    "        with open(f\"{filepath}/{model_file_name}\", 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        logger.info(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath: str):\n",
    "        \"\"\"Load a saved model from disk\"\"\"\n",
    "        import pickle\n",
    "        from scipy import sparse\n",
    "        \n",
    "        # Load the model data\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Create a new instance\n",
    "        instance = cls(\n",
    "            top_n_similar=model_data['top_n_similar'],\n",
    "            cache_size=model_data['cache_size'],\n",
    "            similarity_threshold=model_data['similarity_threshold']\n",
    "        )\n",
    "        \n",
    "        # Load the attributes\n",
    "        instance.user_mapping = model_data['user_mapping']\n",
    "        instance.item_mapping = model_data['item_mapping']\n",
    "        instance.reverse_user_mapping = model_data['reverse_user_mapping']\n",
    "        instance.reverse_item_mapping = model_data['reverse_item_mapping']\n",
    "        instance.item_similarity_matrix = model_data['item_similarity_matrix']\n",
    "        instance.popular_items_cache = model_data['popular_items_cache']\n",
    "        \n",
    "        # Load sparse matrix if it exists\n",
    "        try:\n",
    "            instance.user_item_matrix = sparse.load_npz(f\"{filepath}_user_item_matrix.npz\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"User-item matrix file not found, loading only the similarity data\")\n",
    "        \n",
    "        return instance\n",
    "    \n",
    "    def update_model_with_new_ratings(self, new_ratings_df: pd.DataFrame, \n",
    "                                      recalculate_similarities: bool = False):\n",
    "        \"\"\"\n",
    "        Update the model with new ratings without complete retraining.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        new_ratings_df : pandas.DataFrame\n",
    "            DataFrame containing new user_id, video_id, and rating columns\n",
    "        recalculate_similarities : bool, default=False\n",
    "            Whether to recalculate item similarities (more expensive)\n",
    "        \"\"\"\n",
    "        print(\"Updating model with new ratings...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Track new users and items\n",
    "        new_users = set()\n",
    "        new_items = set()\n",
    "        \n",
    "        # Process each new rating\n",
    "        for _, row in new_ratings_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            video_id = row['video_id']\n",
    "            rating = float(row['rating'])\n",
    "            \n",
    "            # Handle new users\n",
    "            if user_id not in self.user_mapping:\n",
    "                new_users.add(user_id)\n",
    "                user_idx = len(self.user_mapping)\n",
    "                self.user_mapping[user_id] = user_idx\n",
    "                self.reverse_user_mapping[user_idx] = user_id\n",
    "            else:\n",
    "                user_idx = self.user_mapping[user_id]\n",
    "            \n",
    "            # Handle new items\n",
    "            if video_id not in self.item_mapping:\n",
    "                new_items.add(video_id)\n",
    "                item_idx = len(self.item_mapping)\n",
    "                self.item_mapping[video_id] = item_idx\n",
    "                self.reverse_item_mapping[item_idx] = video_id\n",
    "            else:\n",
    "                item_idx = self.item_mapping[video_id]\n",
    "            \n",
    "            # We need to create a new matrix if dimensions changed\n",
    "            if new_users or new_items:\n",
    "                self._rebuild_matrix_with_new_ratings(new_ratings_df)\n",
    "                break\n",
    "            \n",
    "            # Otherwise, update the existing matrix\n",
    "            # For efficiency, only update the matrix, not recalculate similarities\n",
    "            # We'd need to convert to LIL format to update efficiently\n",
    "            lil_matrix = self.user_item_matrix.tolil()\n",
    "            lil_matrix[user_idx, item_idx] = rating\n",
    "            self.user_item_matrix = lil_matrix.tocsr()\n",
    "        \n",
    "        # Recalculate similarities if requested and we have new items\n",
    "        if recalculate_similarities and (new_items or new_users):\n",
    "            self._build_item_similarity_matrix()\n",
    "        \n",
    "        # Refresh popular items cache\n",
    "        self.popular_items_cache = self.recommend_popular_items_wilson()\n",
    "        \n",
    "        # Clear recommendation caches\n",
    "        self.clear_caches()\n",
    "        \n",
    "        print(f\"Model updated in {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"Added {len(new_users)} new users and {len(new_items)} new items\")\n",
    "    \n",
    "    def _rebuild_matrix_with_new_ratings(self, new_ratings_df: pd.DataFrame):\n",
    "        \"\"\"Rebuild the user-item matrix with the new ratings data\"\"\"\n",
    "        n_users = len(self.user_mapping) \n",
    "        n_items = len(self.item_mapping)\n",
    "        \n",
    "        # Create a new matrix with the right dimensions\n",
    "        from scipy.sparse import lil_matrix\n",
    "        new_matrix = lil_matrix((n_users, n_items))\n",
    "        \n",
    "        # Copy existing data if available\n",
    "        if self.user_item_matrix is not None:\n",
    "            # Copy existing values\n",
    "            old_shape = self.user_item_matrix.shape\n",
    "            new_matrix[:old_shape[0], :old_shape[1]] = self.user_item_matrix.tolil()\n",
    "        \n",
    "        # Add new ratings\n",
    "        for _, row in new_ratings_df.iterrows():\n",
    "            user_idx = self.user_mapping[row['user_id']]\n",
    "            item_idx = self.item_mapping[row['video_id']]\n",
    "            rating = float(row['rating'])\n",
    "            new_matrix[user_idx, item_idx] = rating\n",
    "        \n",
    "        # Convert back to CSR for efficiency\n",
    "        self.user_item_matrix = new_matrix.tocsr()\n",
    "        \n",
    "        print(f\"Rebuilt user-item matrix to shape {self.user_item_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(f\"/home/cyrilng/pding-recsys/data/master_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10111 entries, 0 to 10110\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   last_purchased_date  10111 non-null  object \n",
      " 1   trees_consumed       10111 non-null  float64\n",
      " 2   user_id              10111 non-null  object \n",
      " 3   video_id             10111 non-null  object \n",
      " 4   purchase_tier        10111 non-null  object \n",
      " 5   video_duration       10111 non-null  int64  \n",
      " 6   wilson_score         10111 non-null  float64\n",
      " 7   title                10111 non-null  object \n",
      " 8   description          10111 non-null  object \n",
      " 9   rating               10111 non-null  int64  \n",
      " 10  last_updated_date    10111 non-null  object \n",
      " 11  pd_category          10111 non-null  object \n",
      " 12  pd_language          10090 non-null  object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model fitting...\n",
      "Creating user and item mappings...\n",
      "Found 4181 unique users and 2473 unique videos\n",
      "Building user-item matrix...\n",
      "Created user-item matrix of shape (4181, 2473)\n",
      "Building item similarity matrix...\n",
      "Processing batch 2/3...\n",
      "Processing batch 3/3...\n",
      "Item similarity matrix built in 0.81 seconds\n",
      "Item similarity matrix contains 14947 nonzero elements\n",
      "Model fitting completed in 1.40 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cb_recommender = ItemBasedCFRecommender(\n",
    "    top_n_similar=10,\n",
    "    cache_size=1000,\n",
    "    similarity_threshold=0.2\n",
    ")\n",
    "cb_recommender.fit(ratings_df=master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/cyrilng/pding-recsys/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-05-15 00:50:11\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel saved to /home/cyrilng/pding-recsys/models\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cb_recommender.save_model(filepath=model_dir, model_file_name=\"cf_model_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        LK4ocAObQbMskwddEdVzi313cX03\n",
       "1        tfPB9WKdGKQukz3Pqq3aw4MzlD02\n",
       "2        02ivqGblVKYolYmqhJ83xK6BtUE2\n",
       "3        8Y6BS4swAdTXsiitKPN1bAxsdll2\n",
       "4        gF18nqobROdy2gCjoeQhwuCQQEW2\n",
       "                     ...             \n",
       "10106    bBo9vwqsoJMMfLJ3XYxtEA7rhYg2\n",
       "10107    AAwtmPysKBT5ZmckMDSmrlqbJaN2\n",
       "10108    Rq4Q3Q71QbdQ8ucyL6drUqUuAju2\n",
       "10109    Rq4Q3Q71QbdQ8ucyL6drUqUuAju2\n",
       "10110    yUHM3jkmGsO4GzSncBiiRq6oyAR2\n",
       "Name: user_id, Length: 10111, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5788e708-d438-4faa-a38e-ba3f750ab850', np.float64(5.000000000000001)),\n",
       " ('acb490de-c5e8-465f-b37f-bb51b692b327', np.float64(5.000000000000001)),\n",
       " ('926519ab-ee37-4694-a676-444f0486d69c', np.float64(5.000000000000001)),\n",
       " ('3afe179c-cd77-4d89-a1db-639b1f7ea385', np.float64(5.0)),\n",
       " ('810946e3-4b38-4a1c-8a24-fcfba12bd34e', np.float64(5.0)),\n",
       " ('f2f7b429-4023-401d-9848-2c3cf65edcaa', np.float64(5.0)),\n",
       " ('3905ac3e-414f-456b-8b26-1d873fefbb62', np.float64(5.0)),\n",
       " ('880ba8b0-c259-4067-b2c6-f1f4023fd2d3', np.float64(5.0)),\n",
       " ('17c2025a-b79b-4280-84cd-180c9f2100e5', np.float64(5.0)),\n",
       " ('51afa438-f4cb-4819-94e3-717ae45ca0d8', np.float64(5.0))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcm = cb_recommender.recommend_for_user(user_id=\"LK4ocAObQbMskwddEdVzi313cX03\")\n",
    "rcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_recommender.recommend_similar_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MinMaxScaler' object has no attribute 'n_features_in_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnumerical_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features_in_\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MinMaxScaler' object has no attribute 'n_features_in_'"
     ]
    }
   ],
   "source": [
    "numerical_scaler.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10111 entries, 0 to 10110\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   last_purchased_date  10111 non-null  object \n",
      " 1   trees_consumed       10111 non-null  float64\n",
      " 2   user_id              10111 non-null  object \n",
      " 3   video_id             10111 non-null  object \n",
      " 4   purchase_tier        10111 non-null  object \n",
      " 5   video_duration       10111 non-null  int64  \n",
      " 6   wilson_score         10111 non-null  float64\n",
      " 7   title                10111 non-null  object \n",
      " 8   description          10111 non-null  object \n",
      " 9   rating               10111 non-null  int64  \n",
      " 10  last_updated_date    10111 non-null  object \n",
      " 11  pd_category          10111 non-null  object \n",
      " 12  pd_language          10090 non-null  object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/cyrilng/pding-recsys/data/master_df.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
