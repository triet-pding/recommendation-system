{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from mysql.connector import Error\n",
    "import mysql.connector\n",
    "import time\n",
    "import structlog\n",
    "import faiss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = structlog.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MySQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_mysql() -> mysql.connector:\n",
    "    \"\"\"\n",
    "    Creates and returns a connection to the MySQL database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host='pdingproddbreplica.chioko0q2r4e.us-west-1.rds.amazonaws.com',          # Your MySQL server address (localhost for local)\n",
    "            database='pding_prod_db',  # Your database name\n",
    "            user='readonly',      # Your MySQL username\n",
    "            password='welcomePding'   # Your MySQL password\n",
    "            # Uncomment below if needed:\n",
    "            # port=3306,               # MySQL default port is 3306\n",
    "            # auth_plugin='mysql_native_password'  # If using newer MySQL versions\n",
    "        )\n",
    "        \n",
    "        if connection.is_connected():\n",
    "            logger.info(\"Connected to MySQL database\")\n",
    "            return connection\n",
    "            \n",
    "    except Error as e:\n",
    "        logger.info(f\"Error connecting to MySQL: {e}\")\n",
    "        return None\n",
    "    \n",
    "def close_connection(connection: mysql.connector):\n",
    "    \"\"\"\n",
    "    Close the database connection.\n",
    "    \"\"\"\n",
    "    if connection and connection.is_connected():\n",
    "        connection.close()\n",
    "        logger.info(\"MySQL connection closed\")\n",
    "\n",
    "\n",
    "def execute_multiple_queries_with_timing(connection, queries):\n",
    "    \"\"\"\n",
    "    Execute multiple SQL queries sequentially, return results as a list of pandas DataFrames,\n",
    "    and track execution time for each query and the total process.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    total_start_time = time.perf_counter()\n",
    "    try:\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        for idx, query in enumerate(tqdm(queries, desc=\"Executing queries\")):\n",
    "            query_start_time = time.perf_counter()\n",
    "            try:\n",
    "                cursor.execute(query)\n",
    "                records = cursor.fetchall()\n",
    "                df = pd.DataFrame(records)\n",
    "                dataframes.append(df)\n",
    "                query_end_time = time.perf_counter()\n",
    "                logger.info(f\"Query {idx + 1} executed in {query_end_time - query_start_time:.4f} seconds.\")\n",
    "            except Error as e:\n",
    "                logger.info(f\"Error executing query {idx + 1}: {e}\")\n",
    "                dataframes.append(None)\n",
    "        cursor.close()\n",
    "    except Error as e:\n",
    "        logger.info(f\"Error setting up cursor: {e}\")\n",
    "    total_end_time = time.perf_counter()\n",
    "    logger.info(f\"Total execution time: {total_end_time - total_start_time:.4f} seconds.\")\n",
    "    return dataframes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing queries:  25%|██▌       | 1/4 [00:05<00:15,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 executed in 5.1205 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing queries:  50%|█████     | 2/4 [00:06<00:05,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 executed in 1.0814 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing queries:  75%|███████▌  | 3/4 [00:28<00:11, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3 executed in 22.2357 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing queries: 100%|██████████| 4/4 [02:16<00:00, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 4 executed in 108.2194 seconds.\n",
      "Total execution time: 136.8766 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "connection = connect_to_mysql()\n",
    "queries = [\n",
    "    \"select * from videos\",\n",
    "    \"select * from video_rating\",\n",
    "    \"select * from video_purchase\",\n",
    "    \"select * from user_followings\"\n",
    "]\n",
    "dfs = execute_multiple_queries_with_timing(connection, queries)\n",
    "videos_df = dfs[0]\n",
    "video_rating_df = dfs[1]\n",
    "video_purchase_df = dfs[2]\n",
    "user_followings_df = dfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of videos_df: (12214, 24)\n",
      "Shape of video_rating_df: (11686, 5)\n",
      "Shape of video_purchase_df: (202279, 13)\n",
      "Shape of user_followings_df: (1696894, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of videos_df: {videos_df.shape}\")\n",
    "print(f\"Shape of video_rating_df: {video_rating_df.shape}\")\n",
    "print(f\"Shape of video_purchase_df: {video_purchase_df.shape}\")\n",
    "print(f\"Shape of user_followings_df: {user_followings_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = videos_df[videos_df['is_deleted'] == 0]\n",
    "user_followings_df = user_followings_df[user_followings_df['is_deleted'] == 0]\n",
    "video_rating_df['last_updated_date'] = pd.to_datetime(video_rating_df['updated_seconds'], unit='s')\n",
    "\n",
    "# Apply efficient renaming to avoid duplicate columns\n",
    "videos_df = videos_df.rename(columns={'duration': 'video_duration'})\n",
    "video_purchase_df = video_purchase_df.rename(columns={\n",
    "    'last_update_date': 'last_purchased_date',\n",
    "    'duration': 'purchase_tier'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8975 entries, 0 to 12213\n",
      "Data columns (total 24 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   id                   8975 non-null   object        \n",
      " 1   description          8975 non-null   object        \n",
      " 2   video_duration       8975 non-null   int64         \n",
      " 3   is_adult             8975 non-null   int64         \n",
      " 4   is_paid              8975 non-null   int64         \n",
      " 5   is_visible           8975 non-null   int64         \n",
      " 6   status               8975 non-null   object        \n",
      " 7   title                8975 non-null   object        \n",
      " 8   trees                2723 non-null   object        \n",
      " 9   updated_time_stamp   8975 non-null   int64         \n",
      " 10  uploaded_time_stamp  8975 non-null   int64         \n",
      " 11  user_id              8975 non-null   object        \n",
      " 12  video_id             8975 non-null   object        \n",
      " 13  rating_visible       8975 non-null   int64         \n",
      " 14  rating_score         2473 non-null   float64       \n",
      " 15  is_deleted           8975 non-null   int64         \n",
      " 16  is_trailer_on        8975 non-null   int64         \n",
      " 17  trailer_id           2165 non-null   object        \n",
      " 18  is_pinned            85 non-null     float64       \n",
      " 19  pinned_at            85 non-null     datetime64[ns]\n",
      " 20  is_preview_on        8975 non-null   int64         \n",
      " 21  trailer_library_id   8975 non-null   object        \n",
      " 22  video_library_id     8975 non-null   object        \n",
      " 23  drm_enable           406 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int64(10), object(10)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "videos_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import psutil  # You may need to pip install this\n",
    "\n",
    "def process_with_tracking(vp_df, v_df, vr_df, uf_df, chunk_size=5000, output_file=\"outputs.csv\"):\n",
    "    # Set up tracking variables\n",
    "    total_rows = len(vp_df)\n",
    "    rows_processed = 0\n",
    "    chunks_processed = 0\n",
    "    start_time = time.time()\n",
    "    results_saved = 0\n",
    "    \n",
    "    # Create or clear output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"\")  # Just clear the file if it exists\n",
    "    \n",
    "    logger.info(f\"Total rows to process: {total_rows}\")\n",
    "    logger.info(f\"Memory usage at start: {psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Process vp dataframe in chunks\n",
    "    for start_idx in range(0, len(vp_df), chunk_size):\n",
    "        try:\n",
    "            chunk_start_time = time.time()\n",
    "            # Get chunk of video_purchase data\n",
    "            end_idx = min(start_idx + chunk_size, len(vp_df))\n",
    "            vp_chunk = vp_df.iloc[start_idx:end_idx].copy()\n",
    "            \n",
    "            # First join\n",
    "            temp_df = pd.merge(\n",
    "                vp_chunk,\n",
    "                v_df[['video_id', 'video_duration', 'rating_score', 'title', 'description']],\n",
    "                on='video_id',\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Check if any rows remain after first join\n",
    "            if len(temp_df) == 0:\n",
    "                logger.info(f\"Chunk {chunks_processed+1}: No matching rows after first join. Skipping.\")\n",
    "                rows_processed += len(vp_chunk)\n",
    "                chunks_processed += 1\n",
    "                continue\n",
    "                \n",
    "            # Second join\n",
    "            temp_df = pd.merge(\n",
    "                temp_df,\n",
    "                vr_df[['rating', 'video_id', 'user_id', 'last_updated_date']],\n",
    "                on=['user_id', 'video_id'],\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Check if any rows remain after second join\n",
    "            if len(temp_df) == 0:\n",
    "                logger.info(f\"Chunk {chunks_processed+1}: No matching rows after second join. Skipping.\")\n",
    "                rows_processed += len(vp_chunk)\n",
    "                chunks_processed += 1\n",
    "                continue\n",
    "            \n",
    "            # Third join\n",
    "            temp_df = pd.merge(\n",
    "                temp_df,\n",
    "                uf_df[['following', 'pd_category', 'pd_language']],\n",
    "                left_on='video_owner_user_id',\n",
    "                right_on='following',\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Remove duplicates and sort\n",
    "            temp_df = temp_df.drop_duplicates()\n",
    "            \n",
    "            # Append to results file\n",
    "            temp_df.to_csv(output_file, mode='a', header=(results_saved==0), index=False)\n",
    "            results_saved += len(temp_df)\n",
    "            \n",
    "            # Update progress tracking\n",
    "            rows_processed += len(vp_chunk)\n",
    "            chunks_processed += 1\n",
    "            \n",
    "            # Print progress\n",
    "            elapsed = time.time() - start_time\n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            memory_usage = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            logger.info(f\"Chunk {chunks_processed}: Processed {rows_processed}/{total_rows} rows ({rows_processed/total_rows*100:.1f}%)\")\n",
    "            logger.info(f\"  - Rows in this chunk result: {len(temp_df)}\")\n",
    "            logger.info(f\"  - Chunk processing time: {chunk_time:.2f}s\")\n",
    "            logger.info(f\"  - Total elapsed time: {elapsed/60:.1f} minutes\")\n",
    "            logger.info(f\"  - Memory usage: {memory_usage:.2f} MB\")\n",
    "            \n",
    "            # Force garbage collection\n",
    "            del temp_df\n",
    "            del vp_chunk\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error processing chunk {chunks_processed+1}: {str(e)}\")\n",
    "            # Save progress info in case of error\n",
    "            with open(\"processing_error_log.txt\", \"a\") as f:\n",
    "                f.write(f\"Error at chunk {chunks_processed+1}, rows {start_idx}-{end_idx}: {str(e)}\\n\")\n",
    "    \n",
    "    logger.info(f\"\\nProcessing complete:\")\n",
    "    logger.info(f\"  - Total rows processed: {rows_processed}\")\n",
    "    logger.info(f\"  - Total rows in results: {results_saved}\")\n",
    "    logger.info(f\"  - Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    logger.info(f\"  - Results saved to: {output_file}\")\n",
    "    \n",
    "    # Read back the final sorted results if needed\n",
    "    # Note: This might be memory-intensive if the result is very large\n",
    "    logger.info(\"Reading and sorting final results...\")\n",
    "    try:\n",
    "        # Read in chunks and sort\n",
    "        sorted_output_file = \"sorted_\" + output_file\n",
    "        # Read first chunk to get header\n",
    "        first_chunk = pd.read_csv(output_file, nrows=1)\n",
    "        header = list(first_chunk.columns)\n",
    "        \n",
    "        # Write sorted chunks\n",
    "        with open(sorted_output_file, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(','.join(header) + '\\n')\n",
    "        \n",
    "        # Process in chunks for sorting\n",
    "        for chunk in pd.read_csv(output_file, chunksize=100000):\n",
    "            chunk_sorted = chunk.sort_values(by='last_purchased_date', ascending=False)\n",
    "            chunk_sorted.to_csv(sorted_output_file, mode='a', header=False, index=False)\n",
    "        \n",
    "        logger.info(f\"Sorted results saved to: {sorted_output_file}\")\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error during final sorting: {str(e)}\")\n",
    "        logger.info(\"You can still access the unsorted results in the original output file.\")\n",
    "    \n",
    "    return results_saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to process: 202279\n",
      "Memory usage at start: 3237.32 MB\n",
      "Chunk 1: Processed 5000/202279 rows (2.5%)\n",
      "  - Rows in this chunk result: 232\n",
      "  - Chunk processing time: 13.94s\n",
      "  - Total elapsed time: 0.2 minutes\n",
      "  - Memory usage: 3180.89 MB\n",
      "Chunk 2: Processed 10000/202279 rows (4.9%)\n",
      "  - Rows in this chunk result: 222\n",
      "  - Chunk processing time: 12.26s\n",
      "  - Total elapsed time: 0.4 minutes\n",
      "  - Memory usage: 3180.89 MB\n",
      "Chunk 3: Processed 15000/202279 rows (7.4%)\n",
      "  - Rows in this chunk result: 239\n",
      "  - Chunk processing time: 14.25s\n",
      "  - Total elapsed time: 0.7 minutes\n",
      "  - Memory usage: 3180.89 MB\n",
      "Chunk 4: Processed 20000/202279 rows (9.9%)\n",
      "  - Rows in this chunk result: 260\n",
      "  - Chunk processing time: 16.27s\n",
      "  - Total elapsed time: 0.9 minutes\n",
      "  - Memory usage: 3180.89 MB\n",
      "Chunk 5: Processed 25000/202279 rows (12.4%)\n",
      "  - Rows in this chunk result: 244\n",
      "  - Chunk processing time: 14.40s\n",
      "  - Total elapsed time: 1.2 minutes\n",
      "  - Memory usage: 3180.90 MB\n",
      "Chunk 6: Processed 30000/202279 rows (14.8%)\n",
      "  - Rows in this chunk result: 235\n",
      "  - Chunk processing time: 12.82s\n",
      "  - Total elapsed time: 1.4 minutes\n",
      "  - Memory usage: 3180.90 MB\n",
      "Chunk 7: Processed 35000/202279 rows (17.3%)\n",
      "  - Rows in this chunk result: 223\n",
      "  - Chunk processing time: 13.08s\n",
      "  - Total elapsed time: 1.6 minutes\n",
      "  - Memory usage: 3180.90 MB\n",
      "Chunk 8: Processed 40000/202279 rows (19.8%)\n",
      "  - Rows in this chunk result: 267\n",
      "  - Chunk processing time: 14.09s\n",
      "  - Total elapsed time: 1.9 minutes\n",
      "  - Memory usage: 3180.90 MB\n",
      "Chunk 9: Processed 45000/202279 rows (22.2%)\n",
      "  - Rows in this chunk result: 233\n",
      "  - Chunk processing time: 12.56s\n",
      "  - Total elapsed time: 2.1 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 10: Processed 50000/202279 rows (24.7%)\n",
      "  - Rows in this chunk result: 226\n",
      "  - Chunk processing time: 12.13s\n",
      "  - Total elapsed time: 2.3 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 11: Processed 55000/202279 rows (27.2%)\n",
      "  - Rows in this chunk result: 236\n",
      "  - Chunk processing time: 13.63s\n",
      "  - Total elapsed time: 2.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 12: Processed 60000/202279 rows (29.7%)\n",
      "  - Rows in this chunk result: 218\n",
      "  - Chunk processing time: 13.24s\n",
      "  - Total elapsed time: 2.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 13: Processed 65000/202279 rows (32.1%)\n",
      "  - Rows in this chunk result: 239\n",
      "  - Chunk processing time: 14.56s\n",
      "  - Total elapsed time: 3.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 14: Processed 70000/202279 rows (34.6%)\n",
      "  - Rows in this chunk result: 258\n",
      "  - Chunk processing time: 16.07s\n",
      "  - Total elapsed time: 3.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 15: Processed 75000/202279 rows (37.1%)\n",
      "  - Rows in this chunk result: 261\n",
      "  - Chunk processing time: 16.10s\n",
      "  - Total elapsed time: 3.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 16: Processed 80000/202279 rows (39.5%)\n",
      "  - Rows in this chunk result: 242\n",
      "  - Chunk processing time: 16.78s\n",
      "  - Total elapsed time: 3.8 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 17: Processed 85000/202279 rows (42.0%)\n",
      "  - Rows in this chunk result: 241\n",
      "  - Chunk processing time: 15.78s\n",
      "  - Total elapsed time: 4.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 18: Processed 90000/202279 rows (44.5%)\n",
      "  - Rows in this chunk result: 249\n",
      "  - Chunk processing time: 14.99s\n",
      "  - Total elapsed time: 4.3 minutes\n",
      "  - Memory usage: 3212.30 MB\n",
      "Chunk 19: Processed 95000/202279 rows (47.0%)\n",
      "  - Rows in this chunk result: 257\n",
      "  - Chunk processing time: 16.30s\n",
      "  - Total elapsed time: 4.6 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 20: Processed 100000/202279 rows (49.4%)\n",
      "  - Rows in this chunk result: 248\n",
      "  - Chunk processing time: 15.68s\n",
      "  - Total elapsed time: 4.8 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 21: Processed 105000/202279 rows (51.9%)\n",
      "  - Rows in this chunk result: 235\n",
      "  - Chunk processing time: 12.14s\n",
      "  - Total elapsed time: 5.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 22: Processed 110000/202279 rows (54.4%)\n",
      "  - Rows in this chunk result: 236\n",
      "  - Chunk processing time: 13.12s\n",
      "  - Total elapsed time: 5.3 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 23: Processed 115000/202279 rows (56.9%)\n",
      "  - Rows in this chunk result: 227\n",
      "  - Chunk processing time: 12.98s\n",
      "  - Total elapsed time: 5.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 24: Processed 120000/202279 rows (59.3%)\n",
      "  - Rows in this chunk result: 239\n",
      "  - Chunk processing time: 13.37s\n",
      "  - Total elapsed time: 5.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 25: Processed 125000/202279 rows (61.8%)\n",
      "  - Rows in this chunk result: 256\n",
      "  - Chunk processing time: 16.00s\n",
      "  - Total elapsed time: 6.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 26: Processed 130000/202279 rows (64.3%)\n",
      "  - Rows in this chunk result: 254\n",
      "  - Chunk processing time: 14.93s\n",
      "  - Total elapsed time: 6.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 27: Processed 135000/202279 rows (66.7%)\n",
      "  - Rows in this chunk result: 267\n",
      "  - Chunk processing time: 16.03s\n",
      "  - Total elapsed time: 6.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 28: Processed 140000/202279 rows (69.2%)\n",
      "  - Rows in this chunk result: 259\n",
      "  - Chunk processing time: 15.20s\n",
      "  - Total elapsed time: 6.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 29: Processed 145000/202279 rows (71.7%)\n",
      "  - Rows in this chunk result: 254\n",
      "  - Chunk processing time: 15.48s\n",
      "  - Total elapsed time: 7.0 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 30: Processed 150000/202279 rows (74.2%)\n",
      "  - Rows in this chunk result: 223\n",
      "  - Chunk processing time: 12.80s\n",
      "  - Total elapsed time: 7.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 31: Processed 155000/202279 rows (76.6%)\n",
      "  - Rows in this chunk result: 228\n",
      "  - Chunk processing time: 13.17s\n",
      "  - Total elapsed time: 7.4 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 32: Processed 160000/202279 rows (79.1%)\n",
      "  - Rows in this chunk result: 245\n",
      "  - Chunk processing time: 14.97s\n",
      "  - Total elapsed time: 7.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 33: Processed 165000/202279 rows (81.6%)\n",
      "  - Rows in this chunk result: 264\n",
      "  - Chunk processing time: 15.00s\n",
      "  - Total elapsed time: 7.9 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 34: Processed 170000/202279 rows (84.0%)\n",
      "  - Rows in this chunk result: 252\n",
      "  - Chunk processing time: 17.56s\n",
      "  - Total elapsed time: 8.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 35: Processed 175000/202279 rows (86.5%)\n",
      "  - Rows in this chunk result: 242\n",
      "  - Chunk processing time: 14.06s\n",
      "  - Total elapsed time: 8.5 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 36: Processed 180000/202279 rows (89.0%)\n",
      "  - Rows in this chunk result: 255\n",
      "  - Chunk processing time: 14.72s\n",
      "  - Total elapsed time: 8.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 37: Processed 185000/202279 rows (91.5%)\n",
      "  - Rows in this chunk result: 232\n",
      "  - Chunk processing time: 13.71s\n",
      "  - Total elapsed time: 8.9 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 38: Processed 190000/202279 rows (93.9%)\n",
      "  - Rows in this chunk result: 263\n",
      "  - Chunk processing time: 15.14s\n",
      "  - Total elapsed time: 9.2 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 39: Processed 195000/202279 rows (96.4%)\n",
      "  - Rows in this chunk result: 252\n",
      "  - Chunk processing time: 14.83s\n",
      "  - Total elapsed time: 9.4 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 40: Processed 200000/202279 rows (98.9%)\n",
      "  - Rows in this chunk result: 253\n",
      "  - Chunk processing time: 15.59s\n",
      "  - Total elapsed time: 9.7 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "Chunk 41: Processed 202279/202279 rows (100.0%)\n",
      "  - Rows in this chunk result: 102\n",
      "  - Chunk processing time: 5.68s\n",
      "  - Total elapsed time: 9.8 minutes\n",
      "  - Memory usage: 3180.91 MB\n",
      "\n",
      "Processing complete:\n",
      "  - Total rows processed: 202279\n",
      "  - Total rows in results: 9868\n",
      "  - Total time: 9.8 minutes\n",
      "  - Results saved to: outputs.csv\n",
      "Reading and sorting final results...\n",
      "Sorted results saved to: sorted_outputs.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9868"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_with_tracking(\n",
    "    v_df=videos_df,\n",
    "    vp_df=video_purchase_df,\n",
    "    uf_df=user_followings_df,\n",
    "    vr_df=video_rating_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(\"/home/cyrilng/pding-recsys/sorted_outputs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9868 entries, 0 to 9867\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               9868 non-null   object \n",
      " 1   last_purchased_date              9868 non-null   object \n",
      " 2   trees_consumed                   9868 non-null   float64\n",
      " 3   user_id                          9868 non-null   object \n",
      " 4   video_id                         9868 non-null   object \n",
      " 5   video_owner_user_id              9868 non-null   object \n",
      " 6   is_replacement_of_deleted_video  9730 non-null   float64\n",
      " 7   purchase_tier                    9868 non-null   object \n",
      " 8   expiry_date                      9868 non-null   object \n",
      " 9   is_refunded                      9868 non-null   int64  \n",
      " 10  drm_fee                          413 non-null    float64\n",
      " 11  discount_percentage_applied      0 non-null      float64\n",
      " 12  package_purchase_id              0 non-null      float64\n",
      " 13  video_duration                   9868 non-null   int64  \n",
      " 14  rating_score                     9868 non-null   float64\n",
      " 15  title                            9868 non-null   object \n",
      " 16  description                      9868 non-null   object \n",
      " 17  rating                           9868 non-null   int64  \n",
      " 18  last_updated_date                9868 non-null   object \n",
      " 19  following                        9868 non-null   object \n",
      " 20  pd_category                      9831 non-null   object \n",
      " 21  pd_language                      9810 non-null   object \n",
      "dtypes: float64(6), int64(3), object(13)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column and rename\n",
    "master_df.drop(columns=['drm_fee', 'discount_percentage_applied', 'package_purchase_id', \n",
    "                        'is_replacement_of_deleted_video', 'following','is_refunded', \n",
    "                        'expiry_date', 'id', 'video_owner_user_id'], inplace=True)\n",
    "master_df.rename(columns={'rating_score': 'wilson_score'}, inplace=True)\n",
    "master_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 4086\n",
      "Number of unique videos: 2447\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Number of unique users: {master_df['user_id'].nunique()}\")\n",
    "logger.info(f\"Number of unique videos: {master_df['video_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9810 entries, 0 to 9867\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   last_purchased_date  9810 non-null   object \n",
      " 1   trees_consumed       9810 non-null   float64\n",
      " 2   user_id              9810 non-null   object \n",
      " 3   video_id             9810 non-null   object \n",
      " 4   purchase_tier        9810 non-null   object \n",
      " 5   video_duration       9810 non-null   int64  \n",
      " 6   wilson_score         9810 non-null   float64\n",
      " 7   title                9810 non-null   object \n",
      " 8   description          9810 non-null   object \n",
      " 9   rating               9810 non-null   int64  \n",
      " 10  last_updated_date    9810 non-null   object \n",
      " 11  pd_category          9810 non-null   object \n",
      " 12  pd_language          9810 non-null   object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "master_df = master_df.dropna(subset=[\"pd_category\", \"pd_language\"])\n",
    "master_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from konlpy.tag import Okt  # Korean language processor\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedFiltering:\n",
    "    \"\"\"\n",
    "    Content-based filtering recommendation system for items with Korean metadata.\n",
    "    Specifically handles Korean text in 'title' and 'description' attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        #  Initialize Korean text processor\n",
    "        self.okt = Okt() \n",
    "        self.korean_stopwords = self._load_korean_stopwords()\n",
    "        \n",
    "        # Vector database\n",
    "        self.index = None\n",
    "        self.id_mapping = {}\n",
    "        \n",
    "        # Initialize transformers\n",
    "        self.text_vectorizer = None\n",
    "        self.numerical_scaler = MinMaxScaler()\n",
    "        self.categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "        logger.info(\"Content-based Recommender initialized\")\n",
    "        \n",
    "    def _load_korean_stopwords(self):\n",
    "        \"\"\"Load Korean stopwords or use a default set if file not available\"\"\"\n",
    "        logger.info(\"Load Korean stopwords list\")\n",
    "        try:\n",
    "            with open('korean_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "                return set(f.read().splitlines())\n",
    "        except FileNotFoundError:\n",
    "            # Default basic Korean stopwords\n",
    "            return {'이', '그', '저', '것', '수', '등', '들', '및', '에서', '으로', '를', '에', '의', '가', '은', '는', '이런', '저런', '그런'}\n",
    "    \n",
    "    def _tokenize_korean_text(self, text):\n",
    "        \"\"\"Preprocess Korean text with specialized handling\"\"\"\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Normalize text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Korean, English, numbers\n",
    "        text = re.sub(r'[^\\wㄱ-ㅎㅏ-ㅣ가-힣 ]', ' ', text)\n",
    "        \n",
    "        # Tokenize Korean text and select only nouns, adjectives, verbs\n",
    "        tokens = self.okt.pos(text)\n",
    "        filtered_tokens = [word for word, pos in tokens if (pos in ['Noun', 'Adjective', 'Verb'] and \n",
    "                                                           len(word) > 1 and \n",
    "                                                           word not in self.korean_stopwords)]\n",
    "        \n",
    "        return ' '.join(filtered_tokens)\n",
    "\n",
    "    def preprocess_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess Korean text columns (title and description)\"\"\"\n",
    "        logger.info(f\"Preprocessing text for {len(df)} videos\")\n",
    "        # Create copies to avoid modifying the original dataframe\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Tokenize Korean text\n",
    "        df_processed['title_tokenized'] = df_processed['title'].fillna(\"\").apply(self._tokenize_korean_text)\n",
    "        df_processed['description_tokenized'] = df_processed['description'].fillna(\"\").apply(self._tokenize_korean_text)\n",
    "        \n",
    "        # Combine text features\n",
    "        df_processed['text_combined'] = df_processed['title_tokenized'] + \" \" + df_processed['description_tokenized']\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def extract_text_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Extract TF-IDF features from preprocessed text\"\"\"\n",
    "        logger.info(\"Extracting text features\")\n",
    "        if self.text_vectorizer is None:\n",
    "            # Initialize and fit vectorizer if not already done\n",
    "            self.text_vectorizer = TfidfVectorizer(\n",
    "                min_df=2, \n",
    "                max_df=0.95, \n",
    "                max_features=5000, \n",
    "                ngram_range=(1, 2), \n",
    "                sublinear_tf=True\n",
    "            )\n",
    "            text_features = self.text_vectorizer.fit_transform(df['text_combined'])\n",
    "        else:\n",
    "            # Use pre-trained vectorizer\n",
    "            text_features = self.text_vectorizer.transform(df['text_combined'])\n",
    "            \n",
    "        return text_features\n",
    "    \n",
    "    def extract_metadata_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Extract and encode numerical and categorical metadata features\"\"\"\n",
    "        logger.info(\"Extracting metadata features\")\n",
    "        # Handle numerical features\n",
    "        numerical_features = df[['tree_consumed', 'video_duration']].values\n",
    "        scaled_numerical = self.numerical_scaler.fit_transform(numerical_features)\n",
    "        \n",
    "        # Handle categorical features\n",
    "        categorical_features = df[['purchase_tier', 'pd_category']].values\n",
    "        encoded_categorical = self.categorical_encoder.fit_transform(categorical_features).toarray()\n",
    "        \n",
    "        # Combine all metadata features\n",
    "        metadata_features = np.hstack((scaled_numerical, encoded_categorical))\n",
    "        \n",
    "        return metadata_features\n",
    "    \n",
    "    def combine_features(self, text_features: np.ndarray, metadata_features: np.ndarray, \n",
    "                         text_weight: float = 0.7) -> np.ndarray:\n",
    "        \"\"\"Combine text and metadata features with weighting\"\"\"\n",
    "        logger.info(f\"Combining features with text_weight={text_weight}\")\n",
    "        # Normalize feature matrices\n",
    "        text_norm = np.sqrt((text_features.toarray() ** 2).sum(axis=1))\n",
    "        text_normalized = text_features.toarray() / text_norm[:, np.newaxis]\n",
    "        \n",
    "        metadata_norm = np.sqrt((metadata_features ** 2).sum(axis=1))\n",
    "        metadata_normalized = metadata_features / metadata_norm[:, np.newaxis]\n",
    "        \n",
    "        # Combine with weights\n",
    "        combined_features = (text_weight * text_normalized + \n",
    "                             (1 - text_weight) * metadata_normalized)\n",
    "        \n",
    "        return combined_features\n",
    "    \n",
    "    def build_faiss_index(self, feature_matrix: np.ndarray) -> None:\n",
    "        \"\"\"Build FAISS index for fast similarity search\"\"\"\n",
    "        logger.info(f\"Building FAISS index with {feature_matrix.shape[0]} videos\")\n",
    "        \n",
    "        # Convert to float32 as required by FAISS\n",
    "        features_float32 = feature_matrix.astype(np.float32)\n",
    "        \n",
    "        # Create and train index\n",
    "        dimension = features_float32.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "        \n",
    "        # Add vectors to the index\n",
    "        self.index.add(features_float32)\n",
    "        \n",
    "        logger.info(f\"FAISS index built with {self.index.ntotal} vectors\")\n",
    "\n",
    "    def fit(self, video_data: pd.DataFrame) -> None:\n",
    "        \"\"\"Fit the recommendation model on the provided video data\"\"\"\n",
    "        logger.info(f\"Fitting model on {len(video_data)} videos\")\n",
    "        \n",
    "        # Store original video IDs for mapping\n",
    "        original_indices = video_data.index.tolist()\n",
    "        \n",
    "        # Preprocess text\n",
    "        processed_df = self.preprocess_text(video_data)\n",
    "        \n",
    "        # Extract features\n",
    "        text_features = self.extract_text_features(processed_df)\n",
    "        metadata_features = self.extract_metadata_features(processed_df)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = self.combine_features(text_features, metadata_features)\n",
    "        \n",
    "        # Build search index\n",
    "        self.build_faiss_index(combined_features)\n",
    "        \n",
    "        # Create mapping from FAISS index to original video IDs\n",
    "        self.id_mapping = {i: original_indices[i] for i in range(len(original_indices))}\n",
    "        \n",
    "        # Save models\n",
    "        self.save_models()\n",
    "        \n",
    "        logger.info(\"Model fitting completed\")\n",
    "\n",
    "    def find_similar_videos(self, video_id: int, video_data: pd.DataFrame, top_n: int = 10) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Find videos similar to the given video ID\"\"\"\n",
    "        # # Cache key for this query\n",
    "        # cache_key = f\"sim_videos:{video_id}:{top_n}\"\n",
    "        \n",
    "        # # Try to get from cache first\n",
    "        # if self.use_cache:\n",
    "        #     cached_result = self.cache.get(cache_key)\n",
    "        #     if cached_result:\n",
    "        #         CACHE_HIT_COUNTER.inc()\n",
    "        #         logger.info(f\"Cache hit for video_id={video_id}\")\n",
    "        #         return pickle.loads(cached_result)\n",
    "        \n",
    "        logger.info(f\"Finding {top_n} videos similar to video_id={video_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Get the index of the video in our processed data\n",
    "            video_idx = list(self.id_mapping.values()).index(video_id)\n",
    "            \n",
    "            # Get the feature vector for this video\n",
    "            query_vector = np.array([self.index.reconstruct(video_idx)]).astype(np.float32)\n",
    "            \n",
    "            # Search for similar videos\n",
    "            k = top_n + 1  # +1 because the video itself will be included\n",
    "            distances, indices = self.index.search(query_vector, k)\n",
    "            \n",
    "            # Convert to list of (video_id, similarity_score) tuples\n",
    "            # Skip the first result (which is the query video itself)\n",
    "            similar_videos = []\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if self.id_mapping[idx] != video_id:  # Skip the query video\n",
    "                    # Convert distance to similarity score (1 / (1 + distance))\n",
    "                    similarity = 1 / (1 + distances[0][i])\n",
    "                    similar_videos.append((self.id_mapping[idx], float(similarity)))\n",
    "                \n",
    "                if len(similar_videos) == top_n:\n",
    "                    break\n",
    "            \n",
    "            # Cache the result\n",
    "            # if self.use_cache:\n",
    "            #     self.cache.setex(cache_key, self.cache_ttl, pickle.dumps(similar_videos))\n",
    "            \n",
    "            return similar_videos\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error finding similar videos: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "    def save_models(self) -> None:\n",
    "        \"\"\"Save trained models and preprocessors to disk\"\"\"\n",
    "        logger.info(f\"Saving models to {self.model_dir}\")\n",
    "        \n",
    "        # Save text vectorizer\n",
    "        with open(os.path.join(self.model_dir, \"text_vectorizer.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.text_vectorizer, f)\n",
    "        \n",
    "        # Save numerical scaler\n",
    "        with open(os.path.join(self.model_dir, \"numerical_scaler.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.numerical_scaler, f)\n",
    "        \n",
    "        # Save categorical encoder\n",
    "        with open(os.path.join(self.model_dir, \"categorical_encoder.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.categorical_encoder, f)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, os.path.join(self.model_dir, \"faiss_index.bin\"))\n",
    "        \n",
    "        # Save ID mapping\n",
    "        with open(os.path.join(self.model_dir, \"id_mapping.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.id_mapping, f)\n",
    "            \n",
    "        logger.info(\"Models saved successfully\")\n",
    "    \n",
    "    def load_models(self) -> None:\n",
    "        \"\"\"Load trained models and preprocessors from disk\"\"\"\n",
    "        logger.info(f\"Loading models from {self.model_dir}\")\n",
    "        \n",
    "        try:\n",
    "            # Load text vectorizer\n",
    "            with open(os.path.join(self.model_dir, \"text_vectorizer.pkl\"), \"rb\") as f:\n",
    "                self.text_vectorizer = pickle.load(f)\n",
    "            \n",
    "            # Load numerical scaler\n",
    "            with open(os.path.join(self.model_dir, \"numerical_scaler.pkl\"), \"rb\") as f:\n",
    "                self.numerical_scaler = pickle.load(f)\n",
    "            \n",
    "            # Load categorical encoder\n",
    "            with open(os.path.join(self.model_dir, \"categorical_encoder.pkl\"), \"rb\") as f:\n",
    "                self.categorical_encoder = pickle.load(f)\n",
    "            \n",
    "            # Load FAISS index\n",
    "            self.index = faiss.read_index(os.path.join(self.model_dir, \"faiss_index.bin\"))\n",
    "            \n",
    "            # Load ID mapping\n",
    "            with open(os.path.join(self.model_dir, \"id_mapping.pkl\"), \"rb\") as f:\n",
    "                self.id_mapping = pickle.load(f)\n",
    "                \n",
    "            logger.info(\"Models loaded successfully\")\n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading models: {str(e)}\")\n",
    "            return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-to-Item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (11629, 4)\n",
      "Number of unique users: 4831\n",
      "Number of unique videos: 2924\n",
      "Rating distribution:\n",
      "rating\n",
      "5    0.738929\n",
      "1    0.118497\n",
      "4    0.072577\n",
      "3    0.046952\n",
      "2    0.023046\n",
      "Name: proportion, dtype: float64\n",
      "Starting model fitting...\n",
      "Creating user and item mappings...\n",
      "Found 4158 unique users and 2608 unique videos\n",
      "Building user-item matrix...\n",
      "Created user-item matrix of shape (4158, 2608)\n",
      "Building item similarity matrix...\n",
      "Processed 1000/2608 items... (152.02 sec)\n",
      "Processed 2000/2608 items... (310.01 sec)\n",
      "Item similarity matrix built in 411.72 seconds\n",
      "Model fitting completed in 411.74 seconds\n",
      "Model saved to item_cf_model.pkl\n",
      "Test RMSE: 1.118815692958474\n",
      "Top 5 recommendations for user R7DePNyqQDUQOT1MJhIawC2IN323:\n",
      "  Video ee8e8b74-c446-485e-90ab-5285d8b6607c: Score 1.7344\n",
      "  Video e3388292-edde-4a1a-9e7c-5c59973331b2: Score 1.4222\n",
      "  Video b8fd416f-ba7a-4d9c-946d-18e001fbe03f: Score 1.4222\n",
      "  Video 4d13a11e-2206-4a97-a01b-53b9052858d4: Score 1.4222\n",
      "  Video 7274c94e-ca0c-4ea2-8704-0ac3b60bce16: Score 1.4222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "class ItemBasedCF:\n",
    "    \"\"\"\n",
    "    Item-based Collaborative Filtering recommendation system\n",
    "    specifically designed for video recommendations with user ratings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, top_n_similar=10):\n",
    "        \"\"\"\n",
    "        Initialize the recommender system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n_similar : int, default=10\n",
    "            Number of most similar items to store for each item\n",
    "        \"\"\"\n",
    "        self.top_n_similar = top_n_similar\n",
    "        self.user_item_matrix = None\n",
    "        self.item_similarity_matrix = None\n",
    "        self.user_mapping = None\n",
    "        self.item_mapping = None\n",
    "        self.reverse_user_mapping = None\n",
    "        self.reverse_item_mapping = None\n",
    "        \n",
    "    def fit(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Build the item-based collaborative filtering model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ratings_df : pandas.DataFrame\n",
    "            DataFrame containing user_id, video_id, and rating columns\n",
    "        \"\"\"\n",
    "        print(\"Starting model fitting...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create mappings between original IDs and matrix indices\n",
    "        self._create_mappings(ratings_df)\n",
    "        \n",
    "        # Build the user-item matrix\n",
    "        self._build_user_item_matrix(ratings_df)\n",
    "        \n",
    "        # Calculate item similarity matrix\n",
    "        self._build_item_similarity_matrix()\n",
    "        \n",
    "        print(f\"Model fitting completed in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "    def _create_mappings(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"Create mappings between original IDs and matrix indices.\"\"\"\n",
    "        print(\"Creating user and item mappings...\")\n",
    "        \n",
    "        # Get unique users and items\n",
    "        unique_users = ratings_df['user_id'].unique()\n",
    "        unique_items = ratings_df['video_id'].unique()\n",
    "        \n",
    "        # Create mappings\n",
    "        self.user_mapping = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        self.item_mapping = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        \n",
    "        # Create reverse mappings (index to original ID)\n",
    "        self.reverse_user_mapping = {idx: user for user, idx in self.user_mapping.items()}\n",
    "        self.reverse_item_mapping = {idx: item for item, idx in self.item_mapping.items()}\n",
    "        \n",
    "        print(f\"Found {len(unique_users)} unique users and {len(unique_items)} unique videos\")\n",
    "        \n",
    "    def _build_user_item_matrix(self, ratings_df: pd.DataFrame):\n",
    "        \"\"\"Build the user-item matrix from the ratings DataFrame.\"\"\"\n",
    "        print(\"Building user-item matrix...\")\n",
    "        \n",
    "        # Convert IDs to matrix indices\n",
    "        user_indices = [self.user_mapping[user] for user in ratings_df['user_id']]\n",
    "        item_indices = [self.item_mapping[item] for item in ratings_df['video_id']]\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        n_users = len(self.user_mapping)\n",
    "        n_items = len(self.item_mapping)\n",
    "        \n",
    "        # Convert ratings to float values\n",
    "        ratings = ratings_df['rating'].values.astype(float)\n",
    "        \n",
    "        # Create the sparse matrix\n",
    "        self.user_item_matrix = csr_matrix((ratings, (user_indices, item_indices)), \n",
    "                                          shape=(n_users, n_items))\n",
    "        \n",
    "        print(f\"Created user-item matrix of shape {self.user_item_matrix.shape}\")\n",
    "        \n",
    "    def _build_item_similarity_matrix(self):\n",
    "        \"\"\"Calculate the item-item similarity matrix using cosine similarity.\"\"\"\n",
    "        print(\"Building item similarity matrix...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to item-user matrix (transpose)\n",
    "        item_user_matrix = self.user_item_matrix.T.tocsr()\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        n_items = item_user_matrix.shape[0]\n",
    "        self.item_similarity_matrix = {}\n",
    "        \n",
    "        # For each item, calculate similarity with all other items\n",
    "        for i in range(n_items):\n",
    "            # Print progress every 1000 items\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(f\"Processed {i}/{n_items} items... ({time.time() - start_time:.2f} sec)\")\n",
    "            \n",
    "            # Get the item vector\n",
    "            item_vec = item_user_matrix[i].toarray().flatten()\n",
    "            \n",
    "            # Calculate similarities with all items at once\n",
    "            similarities = cosine_similarity(\n",
    "                item_vec.reshape(1, -1), \n",
    "                item_user_matrix.toarray()\n",
    "            ).flatten()\n",
    "            \n",
    "            # Keep only top N similar items (excluding self)\n",
    "            # First, set self-similarity to -1 to exclude it\n",
    "            similarities[i] = -1\n",
    "            \n",
    "            # Get indices of top N items\n",
    "            top_similar_indices = heapq.nlargest(self.top_n_similar, \n",
    "                                                range(len(similarities)), \n",
    "                                                key=similarities.__getitem__)\n",
    "            \n",
    "            # Store only top N similarities per video\n",
    "            self.item_similarity_matrix[i] = {\n",
    "                sim_idx: similarities[sim_idx] \n",
    "                for sim_idx in top_similar_indices \n",
    "                if similarities[sim_idx] > 0\n",
    "            }\n",
    "        \n",
    "        print(f\"Item similarity matrix built in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    def _wilson_score(self, pos, n, confidence=0.95):\n",
    "        \"\"\"\n",
    "        Calculate the Wilson score interval for a binomial proportion.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pos : int or float\n",
    "            Number of positive ratings or sum of ratings\n",
    "        n : int\n",
    "            Total number of ratings\n",
    "        confidence : float, default=0.95\n",
    "            Confidence level\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Lower bound of Wilson score interval\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        \n",
    "        z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
    "        phat = pos / n\n",
    "        \n",
    "        # Wilson score calculation\n",
    "        score = (phat + z*z/(2*n) - z * math.sqrt((phat*(1-phat) + z*z/(4*n))/n)) / (1 + z*z/n)\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def recommend_popular_items_wilson(self, n_recommendations=10, confidence=0.95, normalize_ratings=True):\n",
    "        \"\"\"\n",
    "        Recommend most popular items based on Wilson score.\n",
    "        Used as fallback for cold-start users.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_recommendations : int, default=10\n",
    "            Number of recommendations to generate\n",
    "        confidence : float, default=0.95\n",
    "            Confidence level for Wilson score\n",
    "        normalize_ratings : bool, default=True\n",
    "            Whether to normalize ratings to 0-1 range\n",
    "                \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, wilson_score) tuples\n",
    "        \"\"\"\n",
    "        # Convert user-item matrix to array for easier processing\n",
    "        matrix = self.user_item_matrix.toarray()\n",
    "        \n",
    "        # Prepare to store results\n",
    "        wilson_scores = []\n",
    "        \n",
    "        # Process each item\n",
    "        for item_idx in range(matrix.shape[1]):\n",
    "            # Get ratings for this item\n",
    "            item_ratings = matrix[:, item_idx]\n",
    "            \n",
    "            # Skip items with no ratings\n",
    "            valid_ratings = item_ratings[item_ratings > 0]\n",
    "            if len(valid_ratings) == 0:\n",
    "                wilson_scores.append(0)\n",
    "                continue\n",
    "            \n",
    "            if normalize_ratings:\n",
    "                # Normalize ratings to 0-1 range\n",
    "                # Assuming ratings are 1-5\n",
    "                norm_ratings = (valid_ratings - 1) / 4\n",
    "                pos = np.sum(norm_ratings)\n",
    "            else:\n",
    "                # Use sum of ratings as \"positive\" outcome\n",
    "                pos = np.sum(valid_ratings)\n",
    "            \n",
    "            # Total number of ratings\n",
    "            n = len(valid_ratings)\n",
    "            \n",
    "            # Calculate Wilson score\n",
    "            score = self._wilson_score(pos, n, confidence)\n",
    "            wilson_scores.append(score)\n",
    "        \n",
    "        # Get indices of top items by Wilson score\n",
    "        top_indices = heapq.nlargest(n_recommendations, \n",
    "                                    range(len(wilson_scores)), \n",
    "                                    key=lambda x: wilson_scores[x])\n",
    "        \n",
    "        # Convert to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[idx], wilson_scores[idx])\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def recommend_for_user(self, user_id: int, n_recommendations=10, exclude_watched=True) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Generate personalized recommendations for a user.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id : int or str\n",
    "            Original user ID\n",
    "        n_recommendations : int, default=10\n",
    "            Number of recommendations to generate\n",
    "        exclude_watched : bool, default=True\n",
    "            Whether to exclude videos the user has already watched\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, predicted_rating) tuples\n",
    "        \"\"\"\n",
    "        # Check if user exists in training data\n",
    "        if user_id not in self.user_mapping:\n",
    "            print(f\"User {user_id} not found in training data. Using popular items instead.\")\n",
    "            return self.recommend_popular_items_wilson(n_recommendations)\n",
    "        \n",
    "        # Get user index\n",
    "        user_idx = self.user_mapping[user_id]\n",
    "        \n",
    "        # Get user's ratings\n",
    "        user_ratings = self.user_item_matrix[user_idx].toarray().flatten()\n",
    "        watched_items = np.where(user_ratings > 0)[0]\n",
    "        \n",
    "        if len(watched_items) == 0:\n",
    "            print(f\"User {user_id} has no ratings. Using popular items instead.\")\n",
    "            return self.recommend_popular_items(n_recommendations)\n",
    "        \n",
    "        # Initialize recommendation scores\n",
    "        scores = defaultdict(float)\n",
    "        \n",
    "        # For each rated item\n",
    "        for item_idx in watched_items:\n",
    "            # Get user's rating for this item\n",
    "            item_rating = user_ratings[item_idx]\n",
    "            \n",
    "            # Skip low ratings (optional - you might want to consider negative feedback)\n",
    "            if item_rating < 3:\n",
    "                continue\n",
    "                \n",
    "            # Get similar items\n",
    "            if item_idx in self.item_similarity_matrix:\n",
    "                # For each similar item\n",
    "                for similar_item, similarity in self.item_similarity_matrix[item_idx].items():\n",
    "                    # Skip if user has already watched this item and we want to exclude watched\n",
    "                    if exclude_watched and user_ratings[similar_item] > 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Weight by both rating and similarity\n",
    "                    scores[similar_item] += similarity * item_rating\n",
    "        \n",
    "        # If we have no recommendations after filtering\n",
    "        if len(scores) == 0:\n",
    "            return self.recommend_popular_items(n_recommendations)\n",
    "        \n",
    "        # Sort by score and take top N\n",
    "        top_item_indices = heapq.nlargest(n_recommendations, \n",
    "                                         scores.keys(), \n",
    "                                         key=scores.get)\n",
    "        \n",
    "        # Convert back to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[item_idx], scores[item_idx])\n",
    "            for item_idx in top_item_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def recommend_similar_items(self, video_id, n_recommendations=10) -> List:\n",
    "        \"\"\"\n",
    "        Find videos similar to a given video.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        video_id : int or str\n",
    "            Original video ID\n",
    "        n_recommendations : int, default=10\n",
    "            Number of similar videos to recommend\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        # Check if item exists in training data\n",
    "        if video_id not in self.item_mapping:\n",
    "            print(f\"Video {video_id} not found in training data.\")\n",
    "            return []\n",
    "        \n",
    "        # Get item index\n",
    "        item_idx = self.item_mapping[video_id]\n",
    "        \n",
    "        # If no similarity data for this item\n",
    "        if item_idx not in self.item_similarity_matrix:\n",
    "            print(f\"No similarity data for video {video_id}.\")\n",
    "            return []\n",
    "        \n",
    "        # Get similar items\n",
    "        similar_items = self.item_similarity_matrix[item_idx]\n",
    "        \n",
    "        # Sort by similarity and take top N\n",
    "        top_similar = heapq.nlargest(n_recommendations, \n",
    "                                    similar_items.keys(), \n",
    "                                    key=similar_items.get)\n",
    "        \n",
    "        # Convert back to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[sim_idx], similar_items[sim_idx])\n",
    "            for sim_idx in top_similar\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def recommend_popular_items(self, n_recommendations=10) -> List:\n",
    "        \"\"\"\n",
    "        Recommend most popular items based on average rating.\n",
    "        Used as fallback for cold-start users.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_recommendations : int, default=10\n",
    "            Number of recommendations to generate\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (video_id, avg_rating) tuples\n",
    "        \"\"\"\n",
    "        # Calculate average rating per item\n",
    "        item_means = self.user_item_matrix.mean(axis=0).A1\n",
    "        \n",
    "        # Get indices of top rated items\n",
    "        top_indices = heapq.nlargest(n_recommendations, \n",
    "                                    range(len(item_means)), \n",
    "                                    key=lambda x: item_means[x])\n",
    "        \n",
    "        # Convert to original video IDs\n",
    "        recommendations = [\n",
    "            (self.reverse_item_mapping[idx], item_means[idx])\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the model to a file.\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        model_data = {\n",
    "            'user_mapping': self.user_mapping,\n",
    "            'item_mapping': self.item_mapping,\n",
    "            'reverse_user_mapping': self.reverse_user_mapping,\n",
    "            'reverse_item_mapping': self.reverse_item_mapping,\n",
    "            'user_item_matrix': self.user_item_matrix,\n",
    "            'item_similarity_matrix': self.item_similarity_matrix,\n",
    "            'top_n_similar': self.top_n_similar\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        \"\"\"Load a saved model from a file.\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Create new instance\n",
    "        model = cls(top_n_similar=model_data['top_n_similar'])\n",
    "        \n",
    "        # Restore attributes\n",
    "        model.user_mapping = model_data['user_mapping']\n",
    "        model.item_mapping = model_data['item_mapping']\n",
    "        model.reverse_user_mapping = model_data['reverse_user_mapping']\n",
    "        model.reverse_item_mapping = model_data['reverse_item_mapping']\n",
    "        model.user_item_matrix = model_data['user_item_matrix']\n",
    "        model.item_similarity_matrix = model_data['item_similarity_matrix']\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Example for loading data from CSV file and full pipeline\n",
    "def full_pipeline_example():\n",
    "    \"\"\"\n",
    "    Example showing how to use the ItemBasedCF class with a CSV file.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = video_rating_df\n",
    "    \n",
    "    # Data exploration and preprocessing\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Number of unique users: {df['user_id'].nunique()}\")\n",
    "    print(f\"Number of unique videos: {df['video_id'].nunique()}\")\n",
    "    print(f\"Rating distribution:\\n{df['rating'].value_counts(normalize=True)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"Warning: Dataset contains missing values\")\n",
    "        df = df.dropna()\n",
    "    \n",
    "    # Split into train (80%) and test (20%)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model = ItemBasedCF(top_n_similar=20)\n",
    "    model.fit(train_df)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_model('item_cf_model.pkl')\n",
    "    \n",
    "    # Evaluate the model\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import math\n",
    "    \n",
    "    # Get test user-item pairs\n",
    "    test_users = test_df['user_id'].unique()\n",
    "    \n",
    "    # Calculate RMSE for test set\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        # Get user's test items\n",
    "        user_test_items = test_df[test_df['user_id'] == user_id]\n",
    "        \n",
    "        if user_id not in model.user_mapping:\n",
    "            continue\n",
    "        \n",
    "        user_idx = model.user_mapping[user_id]\n",
    "        user_ratings = model.user_item_matrix[user_idx].toarray().flatten()\n",
    "        watched_items = np.where(user_ratings > 0)[0]\n",
    "        \n",
    "        for _, row in user_test_items.iterrows():\n",
    "            if row['video_id'] not in model.item_mapping:\n",
    "                continue\n",
    "                \n",
    "            item_idx = model.item_mapping[row['video_id']]\n",
    "            actual_rating = row['rating']\n",
    "            \n",
    "            # Predict rating using item-based CF\n",
    "            predicted_rating = 0\n",
    "            total_similarity = 0\n",
    "            \n",
    "            for watched_item in watched_items:\n",
    "                # Skip the current test item\n",
    "                if watched_item == item_idx:\n",
    "                    continue\n",
    "                    \n",
    "                # Get user's rating for this item\n",
    "                rating = user_ratings[watched_item]\n",
    "                \n",
    "                # Get similarity between this item and test item\n",
    "                if watched_item in model.item_similarity_matrix and item_idx in model.item_similarity_matrix[watched_item]:\n",
    "                    similarity = model.item_similarity_matrix[watched_item][item_idx]\n",
    "                    predicted_rating += similarity * rating\n",
    "                    total_similarity += similarity\n",
    "            \n",
    "            # Normalize by total similarity\n",
    "            if total_similarity > 0:\n",
    "                predicted_rating /= total_similarity\n",
    "                \n",
    "                actual_ratings.append(actual_rating)\n",
    "                predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = math.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "    print(f\"Test RMSE: {rmse}\")\n",
    "    \n",
    "    # Generate recommendations for a sample user\n",
    "    sample_user = df['user_id'].iloc[0]\n",
    "    recommendations = model.recommend_for_user(sample_user, n_recommendations=5)\n",
    "    print(f\"Top 5 recommendations for user {sample_user}:\")\n",
    "    for video_id, score in recommendations:\n",
    "        print(f\"  Video {video_id}: Score {score:.4f}\")\n",
    "\n",
    "# Uncomment to run the full pipeline example\n",
    "full_pipeline_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations for a sample user\n",
    "sample_user = df['user_id'].iloc[0]\n",
    "recommendations = model.recommend_for_user(sample_user, n_recommendations=5)\n",
    "print(f\"Top 5 recommendations for user {sample_user}:\")\n",
    "for video_id, score in recommendations:\n",
    "    print(f\"  Video {video_id}: Score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-to-Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "class UserBasedCF:\n",
    "    def __init__(self, n_neighbors=20, min_neighbors=1):\n",
    "        \"\"\"\n",
    "        Initialize User-Based Collaborative Filtering model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_neighbors : int, default=20\n",
    "            Number of neighbors to use for prediction\n",
    "        min_neighbors : int, default=1\n",
    "            Minimum number of neighbors needed to make a prediction\n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_neighbors = min_neighbors\n",
    "        self.user_similarity_matrix = None\n",
    "        self.ratings_matrix = None\n",
    "        self.user_ids = None\n",
    "        self.item_ids = None\n",
    "        self.user_means = None\n",
    "        \n",
    "    def fit(self, ratings_df):\n",
    "        \"\"\"\n",
    "        Fit the model with training data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ratings_df : pandas DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "            The training ratings data\n",
    "        \"\"\"\n",
    "        # Create user-item matrix\n",
    "        \n",
    "        self.ratings_matrix = ratings_df.pivot_table(\n",
    "            index='user_id', \n",
    "            columns='item_id', \n",
    "            values='rating'\n",
    "        ).fillna(0)\n",
    "      \n",
    "        \n",
    "        \n",
    "        self.user_ids = self.ratings_matrix.index.tolist()\n",
    "        self.item_ids = self.ratings_matrix.columns.tolist()\n",
    "        \n",
    "        # Calculate mean rating for each user for later use in prediction\n",
    "        self.user_means = self.ratings_matrix.mean(axis=1)\n",
    "        \n",
    "        # Normalize ratings by subtracting user means\n",
    "        normalized_ratings = self.ratings_matrix.subtract(self.user_means, axis=0)\n",
    "        \n",
    "        # Calculate user similarity matrix using cosine similarity\n",
    "        self.user_similarity_matrix = cosine_similarity(normalized_ratings)\n",
    "        self.user_similarity_matrix = pd.DataFrame(\n",
    "            self.user_similarity_matrix,\n",
    "            index=self.user_ids,\n",
    "            columns=self.user_ids\n",
    "        )\n",
    "        \n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        Predict rating for a single user-item pair\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id : int or str\n",
    "            The user ID\n",
    "        item_id : int or str\n",
    "            The item ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Predicted rating\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_ids:\n",
    "            return self.ratings_matrix.mean().mean()  # Global mean for new users\n",
    "        \n",
    "        if item_id not in self.item_ids:\n",
    "            return self.user_means[user_id]  # User mean for new items\n",
    "        \n",
    "        # Get user's row index\n",
    "        user_idx = self.user_ids.index(user_id)\n",
    "        \n",
    "        # Get all users' similarities to target user\n",
    "        similarities = self.user_similarity_matrix.iloc[user_idx].values\n",
    "        \n",
    "        # Get all users' ratings for the target item\n",
    "        ratings = self.ratings_matrix[item_id].values\n",
    "        \n",
    "        # Mask when ratings are zero (unrated)\n",
    "        mask = ratings != 0\n",
    "        \n",
    "        # Only consider similar users who have rated the item\n",
    "        sims = similarities[mask]\n",
    "        rs = ratings[mask]\n",
    "        \n",
    "        # Check if we have enough neighbors\n",
    "        if len(sims) < self.min_neighbors:\n",
    "            return self.user_means[user_id]\n",
    "        \n",
    "        # Sort by similarity and take top k neighbors\n",
    "        if len(sims) > self.n_neighbors:\n",
    "            idx = np.argsort(sims)[-self.n_neighbors:]\n",
    "            sims = sims[idx]\n",
    "            rs = rs[idx]\n",
    "        \n",
    "        # Normalize ratings by subtracting user means\n",
    "        user_means_array = np.array([self.user_means[uid] for uid in np.array(self.user_ids)[mask]])\n",
    "        if len(user_means_array) > self.n_neighbors:\n",
    "            user_means_array = user_means_array[idx]\n",
    "        \n",
    "        rs_norm = rs - user_means_array\n",
    "        \n",
    "        # Calculate weighted average\n",
    "        if np.sum(np.abs(sims)) > 0:\n",
    "            pred = self.user_means[user_id] + np.sum(sims * rs_norm) / np.sum(np.abs(sims))\n",
    "        else:\n",
    "            pred = self.user_means[user_id]\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def recommend(self, user_id, n_items=10, exclude_rated=True):\n",
    "        \"\"\"\n",
    "        Generate recommendations for a user\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id : int or str\n",
    "            The user ID\n",
    "        n_items : int, default=10\n",
    "            Number of items to recommend\n",
    "        exclude_rated : bool, default=True\n",
    "            Whether to exclude already rated items\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list : List of recommended item IDs\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_ids:\n",
    "            # For new users, recommend most popular items\n",
    "            item_popularity = self.ratings_matrix.sum().sort_values(ascending=False)\n",
    "            return item_popularity.index[:n_items].tolist()\n",
    "        \n",
    "        # Get items the user has already rated\n",
    "        rated_items = self.ratings_matrix.loc[user_id]\n",
    "        rated_items = rated_items[rated_items > 0].index.tolist() if exclude_rated else []\n",
    "        \n",
    "        # Calculate predicted ratings for all unrated items\n",
    "        all_items = [item for item in self.item_ids if item not in rated_items]\n",
    "        predicted_ratings = {item: self.predict(user_id, item) for item in all_items}\n",
    "        \n",
    "        # Sort items by predicted rating\n",
    "        sorted_items = sorted(predicted_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top N recommendations\n",
    "        return [item for item, rating in sorted_items[:n_items]]\n",
    "    \n",
    "    def evaluate(self, test_df):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_df : pandas DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "            The test ratings data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Root Mean Squared Error (RMSE)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        for _, row in test_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            item_id = row['item_id']\n",
    "            \n",
    "            if user_id in self.user_ids and item_id in self.item_ids:\n",
    "                pred = self.predict(user_id, item_id)\n",
    "                predictions.append(pred)\n",
    "                actuals.append(row['rating'])\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = sqrt(mean_squared_error(actuals, predictions))\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2594</td>\n",
       "      <td>1889</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2032</td>\n",
       "      <td>1908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979</td>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>1951</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2156</td>\n",
       "      <td>485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9295</th>\n",
       "      <td>2342</td>\n",
       "      <td>2286</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9296</th>\n",
       "      <td>631</td>\n",
       "      <td>2286</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9297</th>\n",
       "      <td>1746</td>\n",
       "      <td>721</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9298</th>\n",
       "      <td>1746</td>\n",
       "      <td>844</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9299</th>\n",
       "      <td>3798</td>\n",
       "      <td>470</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9271 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id  rating\n",
       "0        2594     1889       5\n",
       "1        2032     1908       1\n",
       "2        1979      448       5\n",
       "3          71     1951       5\n",
       "4        2156      485       1\n",
       "...       ...      ...     ...\n",
       "9295     2342     2286       5\n",
       "9296      631     2286       5\n",
       "9297     1746      721       5\n",
       "9298     1746      844       5\n",
       "9299     3798      470       4\n",
       "\n",
       "[9271 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2i_cf_df = cf_df_encoded[[\"user_idx\", \"item_idx\", \"rating\"]]\n",
    "u2i_cf_df = u2i_cf_df.rename(columns={\"user_idx\": \"user_id\", \"item_idx\":\"item_id\"})\n",
    "u2i_cf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1645/3596032178.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  u2i_cf_df.rename(columns={\"user_idx\": \"user_id\", \"item_idx\":\"item_id\"}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 1: [322, 1160, 1625, 1174, 1341]\n",
      "RMSE: 4.915260353628598\n"
     ]
    }
   ],
   "source": [
    "u2i_cf_df = cf_df_encoded[[\"user_idx\", \"item_idx\", \"rating\"]]\n",
    "u2i_cf_df.rename(columns={\"user_idx\": \"user_id\", \"item_idx\":\"item_id\"}, inplace=True)\n",
    "\n",
    "# Split into train and test (80:20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(u2i_cf_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train model\n",
    "cf_model = UserBasedCF(n_neighbors=2)\n",
    "cf_model.fit(train_df)\n",
    "\n",
    "# Make recommendations for a user\n",
    "recommendations = cf_model.recommend(user_id=1, n_items=5)\n",
    "print(f\"Recommendations for user 1: {recommendations}\")\n",
    "\n",
    "# Evaluate model\n",
    "rmse = cf_model.evaluate(test_df)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>item_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>DNHJJnbYyXd31WUkq0GPXZkkFqr1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>835</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>zNWhroyJfITeNCa9UDijTTvgks63</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3847</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>cpxKW5kyPihjXGPsptgecvot9SE2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>2453</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>yDvMmF1T7NfNKpYXsQms5HkdHFt1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3783</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Ku0YGfIHZuMtSHDmq9yCtEe0DkB3</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1285</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>LhavCaMxdPexiv7AEHdQQGmDU6s1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1344</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>XpVUrvD6WhOEeblPxAlg6wzd5Z92</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>2107</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>SgS7wLNOMoQhWBMx8sQksHWYGdF3</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1798</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>HVgi0BgIVATRrGo66Hi8symnKGy2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1094</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>E7vSNnez82OlQlKEuYumBiyTQ4o2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>878</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>L43Y0iD0KgXyCR0q5kN6u7o5Li33</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1300</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>ib97653dtnXgqeVU2NyiTXu57ap2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>1</td>\n",
       "      <td>2782</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>S5I2AU99q3PnKCEDZFObhZVMjtn1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1760</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>xnJtfvYje0cCY2xeHZFUV1gtBvA3</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3757</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>cIoMpm9xSgbO7Kjy5wdn0zGwkFN2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>2414</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>tjeKvSHYf9NEwIctzH0qHDB5Ozw1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3495</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>8TYdO5PII2W2J4Gk0PDqgzDQjfL2</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>524</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>KCv9lvF5AmOYVrFZpGovyzbaPk53</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1246</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>vsT95uFMBON7lXzhNQCLetqXzTi1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>3634</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>GprKrVkpDPUEqM0fah5rjV8qoxn1</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>1054</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>ZQfXpqhdgQVGFAVsAdILDduDjD23</td>\n",
       "      <td>844b8ae6-f5b9-474c-83fc-9df645de1941</td>\n",
       "      <td>5</td>\n",
       "      <td>2221</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>LhavCaMxdPexiv7AEHdQQGmDU6s1</td>\n",
       "      <td>96499535-2507-427d-bef6-807a05a3535c</td>\n",
       "      <td>5</td>\n",
       "      <td>1344</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>f0gbuiX99XdyDpCW97yZIqwmchH2</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>2573</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>KlneNshPSnONSvlsxgBcBRqhKZG3</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>1278</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>DmzvkB3dQzL7p58SoPhn8RV3Q8x1</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>855</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>H4J1Wy2pPsT6n7vhblSqGUseStt2</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>1068</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6156</th>\n",
       "      <td>kY8OADiRYigje5W2u0fsVfXFNkT2</td>\n",
       "      <td>22da7b5c-8fa4-47e9-84bb-46480cd3e5e6</td>\n",
       "      <td>5</td>\n",
       "      <td>2896</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6244</th>\n",
       "      <td>5JcJDaJXojfFYVaqN6HdRkM1yts1</td>\n",
       "      <td>82e78c44-46f3-462b-a795-ba30e98be9ae</td>\n",
       "      <td>5</td>\n",
       "      <td>352</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td>FqgWj0NlZJdUfF1pAr3KRWmcckx1</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>990</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540</th>\n",
       "      <td>s4UT1Z7mS4ezfHdmMKwcPGVqg0T2</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>4</td>\n",
       "      <td>3383</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>nOZhb970cDOy7ZFAAthuUxytPbR2</td>\n",
       "      <td>b6c2490f-ea3f-454b-8908-a6973dc318dd</td>\n",
       "      <td>5</td>\n",
       "      <td>3048</td>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           user_id                              video_id  \\\n",
       "78    DNHJJnbYyXd31WUkq0GPXZkkFqr1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "188   zNWhroyJfITeNCa9UDijTTvgks63  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "210   cpxKW5kyPihjXGPsptgecvot9SE2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "373   yDvMmF1T7NfNKpYXsQms5HkdHFt1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "410   Ku0YGfIHZuMtSHDmq9yCtEe0DkB3  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "459   LhavCaMxdPexiv7AEHdQQGmDU6s1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "461   XpVUrvD6WhOEeblPxAlg6wzd5Z92  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "476   SgS7wLNOMoQhWBMx8sQksHWYGdF3  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "539   HVgi0BgIVATRrGo66Hi8symnKGy2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "581   E7vSNnez82OlQlKEuYumBiyTQ4o2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "593   L43Y0iD0KgXyCR0q5kN6u7o5Li33  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "635   ib97653dtnXgqeVU2NyiTXu57ap2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "647   S5I2AU99q3PnKCEDZFObhZVMjtn1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "685   xnJtfvYje0cCY2xeHZFUV1gtBvA3  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "691   cIoMpm9xSgbO7Kjy5wdn0zGwkFN2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "713   tjeKvSHYf9NEwIctzH0qHDB5Ozw1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "715   8TYdO5PII2W2J4Gk0PDqgzDQjfL2  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "716   KCv9lvF5AmOYVrFZpGovyzbaPk53  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "722   vsT95uFMBON7lXzhNQCLetqXzTi1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "729   GprKrVkpDPUEqM0fah5rjV8qoxn1  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "731   ZQfXpqhdgQVGFAVsAdILDduDjD23  844b8ae6-f5b9-474c-83fc-9df645de1941   \n",
       "1622  LhavCaMxdPexiv7AEHdQQGmDU6s1  96499535-2507-427d-bef6-807a05a3535c   \n",
       "2324  f0gbuiX99XdyDpCW97yZIqwmchH2  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "3752  KlneNshPSnONSvlsxgBcBRqhKZG3  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "5635  DmzvkB3dQzL7p58SoPhn8RV3Q8x1  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "5910  H4J1Wy2pPsT6n7vhblSqGUseStt2  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "6156  kY8OADiRYigje5W2u0fsVfXFNkT2  22da7b5c-8fa4-47e9-84bb-46480cd3e5e6   \n",
       "6244  5JcJDaJXojfFYVaqN6HdRkM1yts1  82e78c44-46f3-462b-a795-ba30e98be9ae   \n",
       "6490  FqgWj0NlZJdUfF1pAr3KRWmcckx1  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "6540  s4UT1Z7mS4ezfHdmMKwcPGVqg0T2  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "7270  nOZhb970cDOy7ZFAAthuUxytPbR2  b6c2490f-ea3f-454b-8908-a6973dc318dd   \n",
       "\n",
       "      rating  user_idx  item_idx  \n",
       "78         5       835      1174  \n",
       "188        5      3847      1174  \n",
       "210        5      2453      1174  \n",
       "373        5      3783      1174  \n",
       "410        5      1285      1174  \n",
       "459        5      1344      1174  \n",
       "461        5      2107      1174  \n",
       "476        5      1798      1174  \n",
       "539        5      1094      1174  \n",
       "581        5       878      1174  \n",
       "593        5      1300      1174  \n",
       "635        1      2782      1174  \n",
       "647        5      1760      1174  \n",
       "685        5      3757      1174  \n",
       "691        5      2414      1174  \n",
       "713        5      3495      1174  \n",
       "715        5       524      1174  \n",
       "716        5      1246      1174  \n",
       "722        5      3634      1174  \n",
       "729        5      1054      1174  \n",
       "731        5      2221      1174  \n",
       "1622       5      1344      1341  \n",
       "2324       5      2573      1625  \n",
       "3752       5      1278      1625  \n",
       "5635       5       855      1625  \n",
       "5910       5      1068      1625  \n",
       "6156       5      2896       322  \n",
       "6244       5       352      1160  \n",
       "6490       5       990      1625  \n",
       "6540       4      3383      1625  \n",
       "7270       5      3048      1625  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_df_encoded[cf_df_encoded[\"item_idx\"].isin([322, 1160, 1625, 1174, 1341])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
